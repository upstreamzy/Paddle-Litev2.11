2022-08-04 08:39:25,959-WARNING: type object 'QuantizationTransformPass' has no attribute '_supported_quantizable_op_type'
2022-08-04 08:39:25,959-WARNING: If you want to use training-aware and post-training quantization, please use Paddle >= 1.8.4 or develop version
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
AutoScanBaseTest run_and_statis reproduce is None
Start to running test of <class '__main__.TestConv2dOp'>
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187e50d670>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187e528af0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'conv_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'repeated_fc_relu_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'fc_gru_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'fc_fuse_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_27[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_25[label="feed"]
   node_24[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_31[label="fetch"]
   node_30[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_28[label="filter_data"]
   node_26[label="input_data"]
   node_29[label="output_data"]
   node_25->node_24[label="X"]
   node_24->node_26[label="Out"]
   node_28->node_27[label="Filter"]
   node_26->node_27[label="Input"]
   node_27->node_29[label="Output"]
   node_29->node_30[label="X"]
   node_30->node_31[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_67[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_65[label="feed"]
   node_64[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_71[label="fetch"]
   node_70[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_68[label="filter_data"]
   node_66[label="input_data"]
   node_69[label="output_data"]
   node_65->node_64[label="X"]
   node_64->node_66[label="Out"]
   node_68->node_67[label="Filter"]
   node_66->node_67[label="Input"]
   node_67->node_69[label="Output"]
   node_69->node_70[label="X"]
   node_70->node_71[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187e528af0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.544883 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.299039 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x1         1x1x1x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b227df0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187e50d670>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_105[label="feed"]
   node_104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_111[label="fetch"]
   node_110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_108[label="filter_data"]
   node_106[label="input_data"]
   node_109[label="output_data"]
   node_105->node_104[label="X"]
   node_104->node_106[label="Out"]
   node_108->node_107[label="Filter"]
   node_106->node_107[label="Input"]
   node_107->node_109[label="Output"]
   node_109->node_110[label="X"]
   node_110->node_111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_145[label="feed"]
   node_144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_151[label="fetch"]
   node_150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_148[label="filter_data"]
   node_146[label="input_data"]
   node_149[label="output_data"]
   node_145->node_144[label="X"]
   node_144->node_146[label="Out"]
   node_148->node_147[label="Filter"]
   node_146->node_147[label="Input"]
   node_147->node_149[label="Output"]
   node_149->node_150[label="X"]
   node_150->node_151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1d37b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.544883 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.299039 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x1         1x1x1x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (3, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1e3d70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1e3d30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_185[label="feed"]
   node_184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_191[label="fetch"]
   node_190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_188[label="filter_data"]
   node_186[label="input_data"]
   node_189[label="output_data"]
   node_185->node_184[label="X"]
   node_184->node_186[label="Out"]
   node_188->node_187[label="Filter"]
   node_186->node_187[label="Input"]
   node_187->node_189[label="Output"]
   node_189->node_190[label="X"]
   node_190->node_191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_225[label="feed"]
   node_224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_231[label="fetch"]
   node_230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_228[label="filter_data"]
   node_226[label="input_data"]
   node_229[label="output_data"]
   node_225->node_224[label="X"]
   node_224->node_226[label="Out"]
   node_228->node_227[label="Filter"]
   node_226->node_227[label="Input"]
   node_227->node_229[label="Output"]
   node_229->node_230[label="X"]
   node_230->node_231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1e80b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]]],


       [[[0.71518934]]],


       [[[0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 3
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.210438 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             3x1x1x1         1x1x1x1         3x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (3, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187e45c1b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1e8db0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_265[label="feed"]
   node_264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_271[label="fetch"]
   node_270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_268[label="filter_data"]
   node_266[label="input_data"]
   node_269[label="output_data"]
   node_265->node_264[label="X"]
   node_264->node_266[label="Out"]
   node_268->node_267[label="Filter"]
   node_266->node_267[label="Input"]
   node_267->node_269[label="Output"]
   node_269->node_270[label="X"]
   node_270->node_271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {3,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_305[label="feed"]
   node_304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_311[label="fetch"]
   node_310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_308[label="filter_data"]
   node_306[label="input_data"]
   node_309[label="output_data"]
   node_305->node_304[label="X"]
   node_304->node_306[label="Out"]
   node_308->node_307[label="Filter"]
   node_306->node_307[label="Input"]
   node_307->node_309[label="Output"]
   node_309->node_310[label="X"]
   node_310->node_311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1e3730>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]]],


       [[[0.71518934]]],


       [[[0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 3
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.210438 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             3x1x1x1         1x1x1x1         3x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187e4959f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1d30f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_345[label="feed"]
   node_344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_351[label="fetch"]
   node_350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_348[label="filter_data"]
   node_346[label="input_data"]
   node_349[label="output_data"]
   node_345->node_344[label="X"]
   node_344->node_346[label="Out"]
   node_348->node_347[label="Filter"]
   node_346->node_347[label="Input"]
   node_347->node_349[label="Output"]
   node_349->node_350[label="X"]
   node_350->node_351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_385[label="feed"]
   node_384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_391[label="fetch"]
   node_390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_388[label="filter_data"]
   node_386[label="input_data"]
   node_389[label="output_data"]
   node_385->node_384[label="X"]
   node_384->node_386[label="Out"]
   node_388->node_387[label="Filter"]
   node_386->node_387[label="Input"]
   node_387->node_389[label="Output"]
   node_389->node_390[label="X"]
   node_390->node_391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1c4f30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]]],


       [[[0.71518934]]],


       [[[0.60276335]]],


       [[[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 4
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.568045 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.311751 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             4x1x1x1         1x1x1x1         4x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1c44f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187e4959f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_425[label="feed"]
   node_424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_431[label="fetch"]
   node_430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_428[label="filter_data"]
   node_426[label="input_data"]
   node_429[label="output_data"]
   node_425->node_424[label="X"]
   node_424->node_426[label="Out"]
   node_428->node_427[label="Filter"]
   node_426->node_427[label="Input"]
   node_427->node_429[label="Output"]
   node_429->node_430[label="X"]
   node_430->node_431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_465[label="feed"]
   node_464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_471[label="fetch"]
   node_470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_468[label="filter_data"]
   node_466[label="input_data"]
   node_469[label="output_data"]
   node_465->node_464[label="X"]
   node_464->node_466[label="Out"]
   node_468->node_467[label="Filter"]
   node_466->node_467[label="Input"]
   node_467->node_469[label="Output"]
   node_469->node_470[label="X"]
   node_470->node_471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b2bcc70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]]],


       [[[0.71518934]]],


       [[[0.60276335]]],


       [[[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 4
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.568045 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.311751 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             4x1x1x1         1x1x1x1         4x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1f3b70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1f3d70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,50,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_505[label="feed"]
   node_504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_511[label="fetch"]
   node_510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_508[label="filter_data"]
   node_506[label="input_data"]
   node_509[label="output_data"]
   node_505->node_504[label="X"]
   node_504->node_506[label="Out"]
   node_508->node_507[label="Filter"]
   node_506->node_507[label="Input"]
   node_507->node_509[label="Output"]
   node_509->node_510[label="X"]
   node_510->node_511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,50,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_545[label="feed"]
   node_544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_551[label="fetch"]
   node_550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_548[label="filter_data"]
   node_546[label="input_data"]
   node_549[label="output_data"]
   node_545->node_544[label="X"]
   node_544->node_546[label="Out"]
   node_548->node_547[label="Filter"]
   node_546->node_547[label="Input"]
   node_547->node_549[label="Output"]
   node_549->node_550[label="X"]
   node_550->node_551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1f1f70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]],

        [[0.4236548 ]],

        [[0.6458941 ]],

        [[0.4375872 ]],

        [[0.891773  ]],

        [[0.96366274]],

        [[0.3834415 ]],

        [[0.79172504]],

        [[0.5288949 ]],

        [[0.56804454]],

        [[0.92559665]],

        [[0.07103606]],

        [[0.0871293 ]],

        [[0.0202184 ]],

        [[0.83261985]],

        [[0.77815676]],

        [[0.87001216]],

        [[0.9786183 ]],

        [[0.7991586 ]],

        [[0.46147937]],

        [[0.7805292 ]],

        [[0.11827443]],

        [[0.639921  ]],

        [[0.14335328]],

        [[0.9446689 ]],

        [[0.5218483 ]],

        [[0.41466194]],

        [[0.2645556 ]],

        [[0.7742337 ]],

        [[0.45615032]],

        [[0.56843394]],

        [[0.0187898 ]],

        [[0.6176355 ]],

        [[0.6120957 ]],

        [[0.616934  ]],

        [[0.94374806]],

        [[0.6818203 ]],

        [[0.3595079 ]],

        [[0.43703195]],

        [[0.6976312 ]],

        [[0.06022547]],

        [[0.6667667 ]],

        [[0.67063785]],

        [[0.21038257]],

        [[0.12892629]],

        [[0.31542835]],

        [[0.36371076]]],


       [[[0.57019675]],

        [[0.43860152]],

        [[0.9883738 ]],

        [[0.10204481]],

        [[0.20887676]],

        [[0.16130951]],

        [[0.6531083 ]],

        [[0.2532916 ]],

        [[0.46631077]],

        [[0.2444256 ]],

        [[0.15896958]],

        [[0.11037514]],

        [[0.6563296 ]],

        [[0.13818295]],

        [[0.19658236]],

        [[0.36872518]],

        [[0.82099324]],

        [[0.09710128]],

        [[0.8379449 ]],

        [[0.09609841]],

        [[0.97645944]],

        [[0.4686512 ]],

        [[0.9767611 ]],

        [[0.6048455 ]],

        [[0.7392636 ]],

        [[0.03918779]],

        [[0.28280696]],

        [[0.12019656]],

        [[0.2961402 ]],

        [[0.11872772]],

        [[0.31798318]],

        [[0.41426298]],

        [[0.06414749]],

        [[0.6924721 ]],

        [[0.56660146]],

        [[0.2653895 ]],

        [[0.5232481 ]],

        [[0.09394051]],

        [[0.5759465 ]],

        [[0.9292962 ]],

        [[0.31856894]],

        [[0.6674104 ]],

        [[0.13179787]],

        [[0.7163272 ]],

        [[0.2894061 ]],

        [[0.18319136]],

        [[0.5865129 ]],

        [[0.02010755]],

        [[0.82894003]],

        [[0.00469548]]],


       [[[0.6778165 ]],

        [[0.27000797]],

        [[0.735194  ]],

        [[0.96218854]],

        [[0.24875315]],

        [[0.57615733]],

        [[0.5920419 ]],

        [[0.5722519 ]],

        [[0.22308163]],

        [[0.952749  ]],

        [[0.44712538]],

        [[0.84640867]],

        [[0.6994793 ]],

        [[0.29743695]],

        [[0.81379783]],

        [[0.39650574]],

        [[0.8811032 ]],

        [[0.5812729 ]],

        [[0.8817354 ]],

        [[0.6925316 ]],

        [[0.7252543 ]],

        [[0.50132436]],

        [[0.95608366]],

        [[0.6439902 ]],

        [[0.42385504]],

        [[0.6063932 ]],

        [[0.0191932 ]],

        [[0.30157483]],

        [[0.66017354]],

        [[0.2900776 ]],

        [[0.6180154 ]],

        [[0.4287687 ]],

        [[0.13547407]],

        [[0.29828233]],

        [[0.5699649 ]],

        [[0.59087276]],

        [[0.57432526]],

        [[0.6532008 ]],

        [[0.65210325]],

        [[0.43141845]],

        [[0.8965466 ]],

        [[0.36756188]],

        [[0.43586493]],

        [[0.89192337]],

        [[0.806194  ]],

        [[0.7038886 ]],

        [[0.10022689]],

        [[0.9194826 ]],

        [[0.7142413 ]],

        [[0.998847  ]]],


       [[[0.1494483 ]],

        [[0.86812603]],

        [[0.16249293]],

        [[0.6155596 ]],

        [[0.12381998]],

        [[0.8480082 ]],

        [[0.807319  ]],

        [[0.56910074]],

        [[0.4071833 ]],

        [[0.069167  ]],

        [[0.69742876]],

        [[0.45354268]],

        [[0.7220556 ]],

        [[0.8663823 ]],

        [[0.9755215 ]],

        [[0.8558034 ]],

        [[0.01171408]],

        [[0.35997805]],

        [[0.72999054]],

        [[0.17162968]],

        [[0.5210366 ]],

        [[0.05433799]],

        [[0.19999653]],

        [[0.0185218 ]],

        [[0.7936977 ]],

        [[0.22392468]],

        [[0.34535167]],

        [[0.9280813 ]],

        [[0.7044144 ]],

        [[0.03183893]],

        [[0.16469416]],

        [[0.6214784 ]],

        [[0.5772286 ]],

        [[0.23789282]],

        [[0.934214  ]],

        [[0.6139659 ]],

        [[0.5356328 ]],

        [[0.58991   ]],

        [[0.730122  ]],

        [[0.311945  ]],

        [[0.39822108]],

        [[0.20984375]],

        [[0.186193  ]],

        [[0.9443724 ]],

        [[0.73955077]],

        [[0.49045882]],

        [[0.22741462]],

        [[0.25435647]],

        [[0.05802916]],

        [[0.43441662]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 50
the num is 4
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 
0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 0.943748 0.681820 0.359508 0.437032 0.697631 0.060225 0.666767 0.670638 0.210383 0.128926 
0.315428 0.363711 
the weight data is 

0.174658 0.327988 0.680349 0.063208 0.607249 0.477646 0.284000 0.238413 0.514513 0.367928 0.456520 0.337477 0.970494 0.133439 0.096804 0.343392 
0.591027 0.659176 0.397257 0.999278 0.351893 0.721407 0.637583 0.813054 0.976226 0.889794 0.764562 0.698249 0.335498 0.147686 0.062636 0.241902 
0.432281 0.521996 0.773084 0.958741 0.117320 0.107004 0.589695 0.745398 0.848150 0.935832 0.983426 0.399802 0.380335 0.147809 0.684934 0.656762 
0.862063 0.097258 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
14.051379 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             4x50x1x1        1x50x1x1        4x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 14.419417381286621, index : 2.
base=[[[[13.66532 ]]]


 [[[11.444273]]]


 [[[14.419417]]]


 [[[10.912493]]]], 
arr=[[[[1.4051379e+01]]]


 [[[2.6484541e-38]]]


 [[[3.8115318e-43]]]


 [[[0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1fee70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1feef0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,50,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_585[label="feed"]
   node_584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_591[label="fetch"]
   node_590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_588[label="filter_data"]
   node_586[label="input_data"]
   node_589[label="output_data"]
   node_585->node_584[label="X"]
   node_584->node_586[label="Out"]
   node_588->node_587[label="Filter"]
   node_586->node_587[label="Input"]
   node_587->node_589[label="Output"]
   node_589->node_590[label="X"]
   node_590->node_591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,50,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {4,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_625[label="feed"]
   node_624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_631[label="fetch"]
   node_630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_628[label="filter_data"]
   node_626[label="input_data"]
   node_629[label="output_data"]
   node_625->node_624[label="X"]
   node_624->node_626[label="Out"]
   node_628->node_627[label="Filter"]
   node_626->node_627[label="Input"]
   node_627->node_629[label="Output"]
   node_629->node_630[label="X"]
   node_630->node_631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1f3630>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]],

        [[0.4236548 ]],

        [[0.6458941 ]],

        [[0.4375872 ]],

        [[0.891773  ]],

        [[0.96366274]],

        [[0.3834415 ]],

        [[0.79172504]],

        [[0.5288949 ]],

        [[0.56804454]],

        [[0.92559665]],

        [[0.07103606]],

        [[0.0871293 ]],

        [[0.0202184 ]],

        [[0.83261985]],

        [[0.77815676]],

        [[0.87001216]],

        [[0.9786183 ]],

        [[0.7991586 ]],

        [[0.46147937]],

        [[0.7805292 ]],

        [[0.11827443]],

        [[0.639921  ]],

        [[0.14335328]],

        [[0.9446689 ]],

        [[0.5218483 ]],

        [[0.41466194]],

        [[0.2645556 ]],

        [[0.7742337 ]],

        [[0.45615032]],

        [[0.56843394]],

        [[0.0187898 ]],

        [[0.6176355 ]],

        [[0.6120957 ]],

        [[0.616934  ]],

        [[0.94374806]],

        [[0.6818203 ]],

        [[0.3595079 ]],

        [[0.43703195]],

        [[0.6976312 ]],

        [[0.06022547]],

        [[0.6667667 ]],

        [[0.67063785]],

        [[0.21038257]],

        [[0.12892629]],

        [[0.31542835]],

        [[0.36371076]]],


       [[[0.57019675]],

        [[0.43860152]],

        [[0.9883738 ]],

        [[0.10204481]],

        [[0.20887676]],

        [[0.16130951]],

        [[0.6531083 ]],

        [[0.2532916 ]],

        [[0.46631077]],

        [[0.2444256 ]],

        [[0.15896958]],

        [[0.11037514]],

        [[0.6563296 ]],

        [[0.13818295]],

        [[0.19658236]],

        [[0.36872518]],

        [[0.82099324]],

        [[0.09710128]],

        [[0.8379449 ]],

        [[0.09609841]],

        [[0.97645944]],

        [[0.4686512 ]],

        [[0.9767611 ]],

        [[0.6048455 ]],

        [[0.7392636 ]],

        [[0.03918779]],

        [[0.28280696]],

        [[0.12019656]],

        [[0.2961402 ]],

        [[0.11872772]],

        [[0.31798318]],

        [[0.41426298]],

        [[0.06414749]],

        [[0.6924721 ]],

        [[0.56660146]],

        [[0.2653895 ]],

        [[0.5232481 ]],

        [[0.09394051]],

        [[0.5759465 ]],

        [[0.9292962 ]],

        [[0.31856894]],

        [[0.6674104 ]],

        [[0.13179787]],

        [[0.7163272 ]],

        [[0.2894061 ]],

        [[0.18319136]],

        [[0.5865129 ]],

        [[0.02010755]],

        [[0.82894003]],

        [[0.00469548]]],


       [[[0.6778165 ]],

        [[0.27000797]],

        [[0.735194  ]],

        [[0.96218854]],

        [[0.24875315]],

        [[0.57615733]],

        [[0.5920419 ]],

        [[0.5722519 ]],

        [[0.22308163]],

        [[0.952749  ]],

        [[0.44712538]],

        [[0.84640867]],

        [[0.6994793 ]],

        [[0.29743695]],

        [[0.81379783]],

        [[0.39650574]],

        [[0.8811032 ]],

        [[0.5812729 ]],

        [[0.8817354 ]],

        [[0.6925316 ]],

        [[0.7252543 ]],

        [[0.50132436]],

        [[0.95608366]],

        [[0.6439902 ]],

        [[0.42385504]],

        [[0.6063932 ]],

        [[0.0191932 ]],

        [[0.30157483]],

        [[0.66017354]],

        [[0.2900776 ]],

        [[0.6180154 ]],

        [[0.4287687 ]],

        [[0.13547407]],

        [[0.29828233]],

        [[0.5699649 ]],

        [[0.59087276]],

        [[0.57432526]],

        [[0.6532008 ]],

        [[0.65210325]],

        [[0.43141845]],

        [[0.8965466 ]],

        [[0.36756188]],

        [[0.43586493]],

        [[0.89192337]],

        [[0.806194  ]],

        [[0.7038886 ]],

        [[0.10022689]],

        [[0.9194826 ]],

        [[0.7142413 ]],

        [[0.998847  ]]],


       [[[0.1494483 ]],

        [[0.86812603]],

        [[0.16249293]],

        [[0.6155596 ]],

        [[0.12381998]],

        [[0.8480082 ]],

        [[0.807319  ]],

        [[0.56910074]],

        [[0.4071833 ]],

        [[0.069167  ]],

        [[0.69742876]],

        [[0.45354268]],

        [[0.7220556 ]],

        [[0.8663823 ]],

        [[0.9755215 ]],

        [[0.8558034 ]],

        [[0.01171408]],

        [[0.35997805]],

        [[0.72999054]],

        [[0.17162968]],

        [[0.5210366 ]],

        [[0.05433799]],

        [[0.19999653]],

        [[0.0185218 ]],

        [[0.7936977 ]],

        [[0.22392468]],

        [[0.34535167]],

        [[0.9280813 ]],

        [[0.7044144 ]],

        [[0.03183893]],

        [[0.16469416]],

        [[0.6214784 ]],

        [[0.5772286 ]],

        [[0.23789282]],

        [[0.934214  ]],

        [[0.6139659 ]],

        [[0.5356328 ]],

        [[0.58991   ]],

        [[0.730122  ]],

        [[0.311945  ]],

        [[0.39822108]],

        [[0.20984375]],

        [[0.186193  ]],

        [[0.9443724 ]],

        [[0.73955077]],

        [[0.49045882]],

        [[0.22741462]],

        [[0.25435647]],

        [[0.05802916]],

        [[0.43441662]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 50
the num is 4
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 
0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 0.943748 0.681820 0.359508 0.437032 0.697631 0.060225 0.666767 0.670638 0.210383 0.128926 
0.315428 0.363711 
the weight data is 

0.174658 0.327988 0.680349 0.063208 0.607249 0.477646 0.284000 0.238413 0.514513 0.367928 0.456520 0.337477 0.970494 0.133439 0.096804 0.343392 
0.591027 0.659176 0.397257 0.999278 0.351893 0.721407 0.637583 0.813054 0.976226 0.889794 0.764562 0.698249 0.335498 0.147686 0.062636 0.241902 
0.432281 0.521996 0.773084 0.958741 0.117320 0.107004 0.589695 0.745398 0.848150 0.935832 0.983426 0.399802 0.380335 0.147809 0.684934 0.656762 
0.862063 0.097258 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
14.051379 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             4x50x1x1        1x50x1x1        4x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.5286466752489795e+37, index : 2.
base=[[[[13.66532 ]]]


 [[[11.444273]]]


 [[[14.419417]]]


 [[[10.912493]]]], 
arr=[[[[1.4051379e+01]]]


 [[[2.6484541e-38]]]


 [[[1.5286467e+37]]]


 [[[4.5592647e-41]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (50, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1db770>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1db2b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,50,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_665[label="feed"]
   node_664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_671[label="fetch"]
   node_670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_668[label="filter_data"]
   node_666[label="input_data"]
   node_669[label="output_data"]
   node_665->node_664[label="X"]
   node_664->node_666[label="Out"]
   node_668->node_667[label="Filter"]
   node_666->node_667[label="Input"]
   node_667->node_669[label="Output"]
   node_669->node_670[label="X"]
   node_670->node_671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,50,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_705[label="feed"]
   node_704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_711[label="fetch"]
   node_710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_708[label="filter_data"]
   node_706[label="input_data"]
   node_709[label="output_data"]
   node_705->node_704[label="X"]
   node_704->node_706[label="Out"]
   node_708->node_707[label="Filter"]
   node_706->node_707[label="Input"]
   node_707->node_709[label="Output"]
   node_709->node_710[label="X"]
   node_710->node_711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1e8cf0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 4
the num is 1
the wout is 1
the hout is 1
the chout is 50
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 
0.521848 0.414662 0.264556 0.774234 0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 0.943748 0.681820 0.359508 0.437032 0.697631 0.060225 
0.666767 0.670638 0.210383 0.128926 0.315428 0.363711 0.570197 0.438602 0.988374 0.102045 0.208877 0.161310 0.653108 0.253292 0.466311 0.244426 
0.158970 0.110375 0.656330 0.138183 0.196582 0.368725 0.820993 0.097101 0.837945 0.096098 0.976459 0.468651 0.976761 0.604846 0.739264 0.039188 
0.282807 0.120197 0.296140 0.118728 0.317983 0.414263 0.064147 0.692472 0.566601 0.265390 0.523248 0.093941 0.575947 0.929296 0.318569 0.667410 
0.131798 0.716327 0.289406 0.183191 0.586513 0.020108 0.828940 0.004695 0.677817 0.270008 0.735194 0.962189 0.248753 0.576157 0.592042 0.572252 
0.223082 0.952749 0.447125 0.846409 0.699479 0.297437 0.813798 0.396506 0.881103 0.581273 0.881735 0.692532 0.725254 0.501324 0.956084 0.643990 
0.423855 0.606393 0.019193 0.301575 0.660174 0.290078 0.618015 0.428769 0.135474 0.298282 0.569965 0.590873 0.574325 0.653201 0.652103 0.431418 
0.896547 0.367562 0.435865 0.891923 0.806194 0.703889 0.100227 0.919483 0.714241 0.998847 0.149448 0.868126 0.162493 0.615560 0.123820 0.848008 
0.807319 0.569101 0.407183 0.069167 0.697429 0.453543 0.722056 0.866382 0.975522 0.855803 0.011714 0.359978 0.729991 0.171630 0.521037 0.054338 
0.199997 0.018522 0.793698 0.223925 0.345352 0.928081 0.704414 0.031839 0.164694 0.621478 0.577229 0.237893 0.934214 0.613966 0.535633 0.589910 
0.730122 0.311945 0.398221 0.209844 0.186193 0.944372 0.739551 0.490459 0.227415 0.254356 0.058029 0.434417 0.311796 0.696343 0.377752 0.179604 
0.024679 0.067250 0.679393 0.453697 0.536579 0.896671 0.990339 0.216897 
convolution bias_data == NULL
the outptr distance of outptr and odata is 50 
the output data is 
0.906534 1.788694 1.820216 1.005973 30218047113542095043256565550427406336.000000 0.000000 43669863434731176898931799245033832448.000000 0.000000 0.000000 0.000000 0.000000 0.000000 18007318850867757580288.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 22766671145677182992384.000000 33303323197986308096.000000 114208109983892797627792621568.000000 10669668253366890279206912.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 374087361096320444819759431680.000000 33135833237128864595968.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x4x1x1         50x4x1x1        1x50x1x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 4.366986343473118e+37, index : 6.
base=[[[[1.0640206 ]]

  [[1.5496764 ]]

  [[1.8120887 ]]

  [[1.1237175 ]]

  [[1.1642907 ]]

  [[1.0047444 ]]

  [[1.7175202 ]]

  [[0.9631858 ]]

  [[1.0426244 ]]

  [[1.0159137 ]]

  [[0.82921237]]

  [[0.95384455]]

  [[0.6370888 ]]

  [[0.91936886]]

  [[1.3725381 ]]

  [[1.4355925 ]]

  [[0.48436677]]

  [[0.88677216]]

  [[0.86734366]]

  [[1.5363923 ]]

  [[0.8589032 ]]

  [[0.8384801 ]]

  [[1.5325301 ]]

  [[1.2172523 ]]

  [[1.5345309 ]]

  [[1.3031843 ]]

  [[1.8081081 ]]

  [[1.6837628 ]]

  [[0.84219533]]

  [[1.1759186 ]]

  [[0.95318896]]

  [[1.4104962 ]]

  [[1.5036308 ]]

  [[1.5072875 ]]

  [[1.6694593 ]]

  [[1.0661196 ]]

  [[1.1332054 ]]

  [[1.614433  ]]

  [[1.3506477 ]]

  [[0.867046  ]]

  [[0.72343206]]

  [[1.2952312 ]]

  [[1.0124171 ]]

  [[1.596103  ]]

  [[0.97817403]]

  [[1.4906071 ]]

  [[0.57840544]]

  [[0.99469316]]

  [[0.7183651 ]]

  [[1.6508951 ]]]], 
arr=[[[[9.0653384e-01]]

  [[1.7886937e+00]]

  [[1.8202159e+00]]

  [[1.0059730e+00]]

  [[3.0218047e+37]]

  [[7.5728972e-41]]

  [[4.3669863e+37]]

  [[7.0867867e-41]]

  [[4.7855380e-36]]

  [[0.0000000e+00]]

  [[8.4077908e-45]]

  [[0.0000000e+00]]

  [[1.8007319e+22]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[1.2611686e-44]]

  [[9.8090893e-45]]

  [[4.1361176e-36]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[0.0000000e+00]]

  [[1.4012985e-43]]

  [[0.0000000e+00]]

  [[2.2766671e+22]]

  [[3.3303323e+19]]

  [[1.1420811e+29]]

  [[1.0669668e+25]]

  [[7.4341451e-36]]

  [[0.0000000e+00]]

  [[1.8777399e-43]]

  [[0.0000000e+00]]

  [[5.3449504e-36]]

  [[0.0000000e+00]]

  [[6.2812911e-36]]

  [[0.0000000e+00]]

  [[3.7408736e+29]]

  [[3.3135833e+22]]

  [[1.4331248e-38]]

  [[0.0000000e+00]]

  [[1.1070258e-43]]

  [[0.0000000e+00]]

  [[1.5134023e-43]]

  [[0.0000000e+00]]

  [[3.5451673e-36]]

  [[0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 50, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1e8470>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1e85f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,50,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,50,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_745[label="feed"]
   node_744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_751[label="fetch"]
   node_750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_748[label="filter_data"]
   node_746[label="input_data"]
   node_749[label="output_data"]
   node_745->node_744[label="X"]
   node_744->node_746[label="Out"]
   node_748->node_747[label="Filter"]
   node_746->node_747[label="Input"]
   node_747->node_749[label="Output"]
   node_749->node_750[label="X"]
   node_750->node_751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,50,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,50,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_785[label="feed"]
   node_784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_791[label="fetch"]
   node_790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_788[label="filter_data"]
   node_786[label="input_data"]
   node_789[label="output_data"]
   node_785->node_784[label="X"]
   node_784->node_786[label="Out"]
   node_788->node_787[label="Filter"]
   node_786->node_787[label="Input"]
   node_787->node_789[label="Output"]
   node_789->node_790[label="X"]
   node_790->node_791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af07930>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934],
         [0.60276335],
         [0.5448832 ],
         [0.4236548 ],
         [0.6458941 ],
         [0.4375872 ],
         [0.891773  ],
         [0.96366274],
         [0.3834415 ],
         [0.79172504],
         [0.5288949 ],
         [0.56804454],
         [0.92559665],
         [0.07103606],
         [0.0871293 ],
         [0.0202184 ],
         [0.83261985],
         [0.77815676],
         [0.87001216],
         [0.9786183 ],
         [0.7991586 ],
         [0.46147937],
         [0.7805292 ],
         [0.11827443],
         [0.639921  ],
         [0.14335328],
         [0.9446689 ],
         [0.5218483 ],
         [0.41466194],
         [0.2645556 ],
         [0.7742337 ],
         [0.45615032],
         [0.56843394],
         [0.0187898 ],
         [0.6176355 ],
         [0.6120957 ],
         [0.616934  ],
         [0.94374806],
         [0.6818203 ],
         [0.3595079 ],
         [0.43703195],
         [0.6976312 ],
         [0.06022547],
         [0.6667667 ],
         [0.67063785],
         [0.21038257],
         [0.12892629],
         [0.31542835],
         [0.36371076]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 50
the chin is 1
the num is 1
the wout is 1
the hout is 50
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 
0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 0.943748 0.681820 0.359508 0.437032 0.697631 0.060225 0.666767 0.670638 0.210383 0.128926 
0.315428 0.363711 
the weight data is 

0.149448 0.868126 0.162493 0.615560 
convolution bias_data == NULL
the outptr distance of outptr and odata is 200 
the output data is 
0.082019 0.620874 0.097945 0.335408 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 185112.562500 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.461479 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 123533485577193572532224.000000 42948400217398163689462283894784.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 5778449498618544684793856.000000 0.000000 0.000000 0.000000 46.832260 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2754617540608.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x50x1        4x1x1x1         1x4x50x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 50, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187e4b7f30>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1d3b70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,50,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,50,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_825[label="feed"]
   node_824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_831[label="fetch"]
   node_830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_828[label="filter_data"]
   node_826[label="input_data"]
   node_829[label="output_data"]
   node_825->node_824[label="X"]
   node_824->node_826[label="Out"]
   node_828->node_827[label="Filter"]
   node_826->node_827[label="Input"]
   node_827->node_829[label="Output"]
   node_829->node_830[label="X"]
   node_830->node_831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,50,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,50,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_865[label="feed"]
   node_864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_871[label="fetch"]
   node_870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_868[label="filter_data"]
   node_866[label="input_data"]
   node_869[label="output_data"]
   node_865->node_864[label="X"]
   node_864->node_866[label="Out"]
   node_868->node_867[label="Filter"]
   node_866->node_867[label="Input"]
   node_867->node_869[label="Output"]
   node_869->node_870[label="X"]
   node_870->node_871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aeffef0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934],
         [0.60276335],
         [0.5448832 ],
         [0.4236548 ],
         [0.6458941 ],
         [0.4375872 ],
         [0.891773  ],
         [0.96366274],
         [0.3834415 ],
         [0.79172504],
         [0.5288949 ],
         [0.56804454],
         [0.92559665],
         [0.07103606],
         [0.0871293 ],
         [0.0202184 ],
         [0.83261985],
         [0.77815676],
         [0.87001216],
         [0.9786183 ],
         [0.7991586 ],
         [0.46147937],
         [0.7805292 ],
         [0.11827443],
         [0.639921  ],
         [0.14335328],
         [0.9446689 ],
         [0.5218483 ],
         [0.41466194],
         [0.2645556 ],
         [0.7742337 ],
         [0.45615032],
         [0.56843394],
         [0.0187898 ],
         [0.6176355 ],
         [0.6120957 ],
         [0.616934  ],
         [0.94374806],
         [0.6818203 ],
         [0.3595079 ],
         [0.43703195],
         [0.6976312 ],
         [0.06022547],
         [0.6667667 ],
         [0.67063785],
         [0.21038257],
         [0.12892629],
         [0.31542835],
         [0.36371076]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 50
the chin is 1
the num is 1
the wout is 1
the hout is 50
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 
0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 0.943748 0.681820 0.359508 0.437032 0.697631 0.060225 0.666767 0.670638 0.210383 0.128926 
0.315428 0.363711 
the weight data is 

0.149448 0.868126 0.162493 0.615560 
convolution bias_data == NULL
the outptr distance of outptr and odata is 200 
the output data is 
0.082019 0.620874 0.097945 0.335408 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.461479 167543895972435568607677644800.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 141314878789553835865628452323328.000000 7899142461563830283540711342080.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 4306089762112535527424.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x50x1        4x1x1x1         1x4x50x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1e8af0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af02830>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_905[label="feed"]
   node_904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_911[label="fetch"]
   node_910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_908[label="filter_data"]
   node_906[label="input_data"]
   node_909[label="output_data"]
   node_905->node_904[label="X"]
   node_904->node_906[label="Out"]
   node_908->node_907[label="Filter"]
   node_906->node_907[label="Input"]
   node_907->node_909[label="Output"]
   node_909->node_910[label="X"]
   node_910->node_911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_945[label="feed"]
   node_944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_951[label="fetch"]
   node_950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_948[label="filter_data"]
   node_946[label="input_data"]
   node_949[label="output_data"]
   node_945->node_944[label="X"]
   node_944->node_946[label="Out"]
   node_948->node_947[label="Filter"]
   node_946->node_947[label="Input"]
   node_947->node_949[label="Output"]
   node_949->node_950[label="X"]
   node_950->node_951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af14a70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 4
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.906534 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x4x1x1         1x4x1x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af146f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1e8af0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_985[label="feed"]
   node_984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_991[label="fetch"]
   node_990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_988[label="filter_data"]
   node_986[label="input_data"]
   node_989[label="output_data"]
   node_985->node_984[label="X"]
   node_984->node_986[label="Out"]
   node_988->node_987[label="Filter"]
   node_986->node_987[label="Input"]
   node_987->node_989[label="Output"]
   node_989->node_990[label="X"]
   node_990->node_991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1025[label="feed"]
   node_1024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1031[label="fetch"]
   node_1030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1028[label="filter_data"]
   node_1026[label="input_data"]
   node_1029[label="output_data"]
   node_1025->node_1024[label="X"]
   node_1024->node_1026[label="Out"]
   node_1028->node_1027[label="Filter"]
   node_1026->node_1027[label="Input"]
   node_1027->node_1029[label="Output"]
   node_1029->node_1030[label="X"]
   node_1030->node_1031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aeff870>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 4
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.906534 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x4x1x1         1x4x1x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af1fef0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af1feb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1067[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1065[label="feed"]
   node_1064[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1071[label="fetch"]
   node_1070[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1068[label="filter_data"]
   node_1066[label="input_data"]
   node_1069[label="output_data"]
   node_1065->node_1064[label="X"]
   node_1064->node_1066[label="Out"]
   node_1068->node_1067[label="Filter"]
   node_1066->node_1067[label="Input"]
   node_1067->node_1069[label="Output"]
   node_1069->node_1070[label="X"]
   node_1070->node_1071[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1105[label="feed"]
   node_1104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1111[label="fetch"]
   node_1110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1108[label="filter_data"]
   node_1106[label="input_data"]
   node_1109[label="output_data"]
   node_1105->node_1104[label="X"]
   node_1104->node_1106[label="Out"]
   node_1108->node_1107[label="Filter"]
   node_1106->node_1107[label="Input"]
   node_1107->node_1109[label="Output"]
   node_1109->node_1110[label="X"]
   node_1110->node_1111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af20870>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 4
the num is 1
the wout is 1
the hout is 1
the chout is 2
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
0.906534 1.788694 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x4x1x1         2x4x1x1         1x2x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af204b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af1fef0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1145[label="feed"]
   node_1144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1151[label="fetch"]
   node_1150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1148[label="filter_data"]
   node_1146[label="input_data"]
   node_1149[label="output_data"]
   node_1145->node_1144[label="X"]
   node_1144->node_1146[label="Out"]
   node_1148->node_1147[label="Filter"]
   node_1146->node_1147[label="Input"]
   node_1147->node_1149[label="Output"]
   node_1149->node_1150[label="X"]
   node_1150->node_1151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1185[label="feed"]
   node_1184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1191[label="fetch"]
   node_1190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1188[label="filter_data"]
   node_1186[label="input_data"]
   node_1189[label="output_data"]
   node_1185->node_1184[label="X"]
   node_1184->node_1186[label="Out"]
   node_1188->node_1187[label="Filter"]
   node_1186->node_1187[label="Input"]
   node_1187->node_1189[label="Output"]
   node_1189->node_1190[label="X"]
   node_1190->node_1191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea59b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 4
the num is 1
the wout is 1
the hout is 1
the chout is 2
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
0.906534 1.788694 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x4x1x1         2x4x1x1         1x2x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (18, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aea7e70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aea73f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,18,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1225[label="feed"]
   node_1224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1231[label="fetch"]
   node_1230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1228[label="filter_data"]
   node_1226[label="input_data"]
   node_1229[label="output_data"]
   node_1225->node_1224[label="X"]
   node_1224->node_1226[label="Out"]
   node_1228->node_1227[label="Filter"]
   node_1226->node_1227[label="Input"]
   node_1227->node_1229[label="Output"]
   node_1229->node_1230[label="X"]
   node_1230->node_1231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,18,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1265[label="feed"]
   node_1264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1271[label="fetch"]
   node_1270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1268[label="filter_data"]
   node_1266[label="input_data"]
   node_1269[label="output_data"]
   node_1265->node_1264[label="X"]
   node_1264->node_1266[label="Out"]
   node_1268->node_1267[label="Filter"]
   node_1266->node_1267[label="Input"]
   node_1267->node_1269[label="Output"]
   node_1269->node_1270[label="X"]
   node_1270->node_1271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea7ab0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]],

        [[0.60276335]],

        [[0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 4
the num is 1
the wout is 1
the hout is 1
the chout is 18
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 
0.521848 0.414662 0.264556 0.774234 0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 0.943748 0.681820 0.359508 0.437032 0.697631 0.060225 
0.666767 0.670638 0.210383 0.128926 0.315428 0.363711 0.570197 0.438602 0.988374 0.102045 0.208877 0.161310 0.653108 0.253292 0.466311 0.244426 
0.158970 0.110375 0.656330 0.138183 0.196582 0.368725 0.820993 0.097101 0.837945 0.096098 0.976459 0.468651 0.976761 0.604846 0.739264 0.039188 
0.282807 0.120197 0.296140 0.118728 0.317983 0.414263 0.064147 0.692472 
convolution bias_data == NULL
the outptr distance of outptr and odata is 18 
the output data is 
0.906534 1.788694 1.820216 1.005973 33434795691192600231936.000000 0.000000 0.000000 0.000066 25650288746824110106879055976212725760.000000 0.000000 22344448664320122518644681515479334912.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
13843654301070907146240.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x4x1x1         18x4x1x1        1x18x1x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.565028874682411e+37, index : 8.
base=[[[[1.0640206 ]]

  [[1.5496764 ]]

  [[1.8120887 ]]

  [[1.1237175 ]]

  [[1.1642907 ]]

  [[1.0047444 ]]

  [[1.7175202 ]]

  [[0.9631858 ]]

  [[1.0426244 ]]

  [[1.0159137 ]]

  [[0.82921237]]

  [[0.95384455]]

  [[0.6370888 ]]

  [[0.91936886]]

  [[1.3725381 ]]

  [[1.4355925 ]]

  [[0.48436677]]

  [[0.88677216]]]], 
arr=[[[[9.0653384e-01]]

  [[1.7886937e+00]]

  [[1.8202159e+00]]

  [[1.0059730e+00]]

  [[3.3434796e+22]]

  [[5.9584612e-41]]

  [[0.0000000e+00]]

  [[6.5743639e-05]]

  [[2.5650289e+37]]

  [[7.6957910e-41]]

  [[2.2344449e+37]]

  [[7.3730720e-41]]

  [[3.2252920e-36]]

  [[0.0000000e+00]]

  [[1.2611686e-44]]

  [[0.0000000e+00]]

  [[1.3843654e+22]]

  [[0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (15, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af20070>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af200b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,15,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1305[label="feed"]
   node_1304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1311[label="fetch"]
   node_1310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1308[label="filter_data"]
   node_1306[label="input_data"]
   node_1309[label="output_data"]
   node_1305->node_1304[label="X"]
   node_1304->node_1306[label="Out"]
   node_1308->node_1307[label="Filter"]
   node_1306->node_1307[label="Input"]
   node_1307->node_1309[label="Output"]
   node_1309->node_1310[label="X"]
   node_1310->node_1311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,15,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1345[label="feed"]
   node_1344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1351[label="fetch"]
   node_1350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1348[label="filter_data"]
   node_1346[label="input_data"]
   node_1349[label="output_data"]
   node_1345->node_1344[label="X"]
   node_1344->node_1346[label="Out"]
   node_1348->node_1347[label="Filter"]
   node_1346->node_1347[label="Input"]
   node_1347->node_1349[label="Output"]
   node_1349->node_1350[label="X"]
   node_1350->node_1351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af191b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 15
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 
convolution bias_data == NULL
the outptr distance of outptr and odata is 15 
the output data is 
0.299039 0.000000 0.000000 0.000000 15094544860171758206976.000000 0.000000 0.000000 0.000000 8091011302191215806918481356647301120.000000 0.000000 14159849887440557105587202302937137152.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x1         15x1x1x1        1x15x1x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (15, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af19c70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af20070>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,15,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1385[label="feed"]
   node_1384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1391[label="fetch"]
   node_1390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1388[label="filter_data"]
   node_1386[label="input_data"]
   node_1389[label="output_data"]
   node_1385->node_1384[label="X"]
   node_1384->node_1386[label="Out"]
   node_1388->node_1387[label="Filter"]
   node_1386->node_1387[label="Input"]
   node_1387->node_1389[label="Output"]
   node_1389->node_1390[label="X"]
   node_1390->node_1391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,15,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1425[label="feed"]
   node_1424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1431[label="fetch"]
   node_1430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1428[label="filter_data"]
   node_1426[label="input_data"]
   node_1429[label="output_data"]
   node_1425->node_1424[label="X"]
   node_1424->node_1426[label="Out"]
   node_1428->node_1427[label="Filter"]
   node_1426->node_1427[label="Input"]
   node_1427->node_1429[label="Output"]
   node_1429->node_1430[label="X"]
   node_1430->node_1431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af20070>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 15
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 
convolution bias_data == NULL
the outptr distance of outptr and odata is 15 
the output data is 
0.299039 0.000000 0.000000 0.000000 15094544860171758206976.000000 0.000000 0.000000 0.000000 8091011302191215806918481356647301120.000000 0.000000 14159849887440557105587202302937137152.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x1         15x1x1x1        1x15x1x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (16, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af3b7b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187e50d670>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,16,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1465[label="feed"]
   node_1464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1471[label="fetch"]
   node_1470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1468[label="filter_data"]
   node_1466[label="input_data"]
   node_1469[label="output_data"]
   node_1465->node_1464[label="X"]
   node_1464->node_1466[label="Out"]
   node_1468->node_1467[label="Filter"]
   node_1466->node_1467[label="Input"]
   node_1467->node_1469[label="Output"]
   node_1469->node_1470[label="X"]
   node_1470->node_1471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,16,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1505[label="feed"]
   node_1504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1511[label="fetch"]
   node_1510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1508[label="filter_data"]
   node_1506[label="input_data"]
   node_1509[label="output_data"]
   node_1505->node_1504[label="X"]
   node_1504->node_1506[label="Out"]
   node_1508->node_1507[label="Filter"]
   node_1506->node_1507[label="Input"]
   node_1507->node_1509[label="Output"]
   node_1509->node_1510[label="X"]
   node_1510->node_1511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af14db0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 16
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 
0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 0.456150 0.568434 0.018790 0.617635 0.612096 0.616934 
convolution bias_data == NULL
the outptr distance of outptr and odata is 16 
the output data is 
0.729571 0.963435 20187612156465402972844524826911571968.000000 0.000000 0.000000 0.000000 0.000000 0.000000 21022787297966779531264.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         16x2x1x1        1x16x1x1        0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.0187612156465403e+37, index : 2.
base=[[[[0.8779403 ]]

  [[0.8031044 ]]

  [[0.8127694 ]]

  [[0.9737274 ]]

  [[0.10129949]]

  [[0.606577  ]]

  [[1.0492864 ]]

  [[1.1086286 ]]

  [[0.81149226]]

  [[0.52257526]]

  [[0.75429136]]

  [[0.58295923]]

  [[0.69891536]]

  [[0.65687937]]

  [[0.4520384 ]]

  [[0.77715105]]]], 
arr=[[[[7.2957087e-01]]

  [[9.6343458e-01]]

  [[2.0187612e+37]]

  [[6.8098901e-41]]

  [[4.9604446e-37]]

  [[0.0000000e+00]]

  [[8.4077908e-45]]

  [[0.0000000e+00]]

  [[2.1022787e+22]]

  [[0.0000000e+00]]

  [[3.3214345e-36]]

  [[0.0000000e+00]]

  [[1.6815582e-44]]

  [[7.0064923e-45]]

  [[1.9959459e-36]]

  [[0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (9, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af02fb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af026f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,9,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1545[label="feed"]
   node_1544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1551[label="fetch"]
   node_1550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1548[label="filter_data"]
   node_1546[label="input_data"]
   node_1549[label="output_data"]
   node_1545->node_1544[label="X"]
   node_1544->node_1546[label="Out"]
   node_1548->node_1547[label="Filter"]
   node_1546->node_1547[label="Input"]
   node_1547->node_1549[label="Output"]
   node_1549->node_1550[label="X"]
   node_1550->node_1551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,9,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1585[label="feed"]
   node_1584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1591[label="fetch"]
   node_1590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1588[label="filter_data"]
   node_1586[label="input_data"]
   node_1589[label="output_data"]
   node_1585->node_1584[label="X"]
   node_1584->node_1586[label="Out"]
   node_1588->node_1587[label="Filter"]
   node_1586->node_1587[label="Input"]
   node_1587->node_1589[label="Output"]
   node_1589->node_1590[label="X"]
   node_1590->node_1591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af187f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 9
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 
0.461479 0.780529 
convolution bias_data == NULL
the outptr distance of outptr and odata is 9 
the output data is 
0.729571 0.963435 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 21022787297966779531264.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         9x2x1x1         1x9x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.102278729796678e+22, index : 8.
base=[[[[0.8779403 ]]

  [[0.8031044 ]]

  [[0.8127694 ]]

  [[0.9737274 ]]

  [[0.10129949]]

  [[0.606577  ]]

  [[1.0492864 ]]

  [[1.1086286 ]]

  [[0.8114923 ]]]], 
arr=[[[[7.2957087e-01]]

  [[9.6343458e-01]]

  [[5.0306615e-43]]

  [[0.0000000e+00]]

  [[4.7129627e-37]]

  [[0.0000000e+00]]

  [[8.4077908e-45]]

  [[0.0000000e+00]]

  [[2.1022787e+22]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af19270>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af19f70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1625[label="feed"]
   node_1624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1631[label="fetch"]
   node_1630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1628[label="filter_data"]
   node_1626[label="input_data"]
   node_1629[label="output_data"]
   node_1625->node_1624[label="X"]
   node_1624->node_1626[label="Out"]
   node_1628->node_1627[label="Filter"]
   node_1626->node_1627[label="Input"]
   node_1627->node_1629[label="Output"]
   node_1629->node_1630[label="X"]
   node_1630->node_1631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1665[label="feed"]
   node_1664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1671[label="fetch"]
   node_1670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1668[label="filter_data"]
   node_1666[label="input_data"]
   node_1669[label="output_data"]
   node_1665->node_1664[label="X"]
   node_1664->node_1666[label="Out"]
   node_1668->node_1667[label="Filter"]
   node_1666->node_1667[label="Input"]
   node_1667->node_1669[label="Output"]
   node_1669->node_1670[label="X"]
   node_1670->node_1671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af1fe70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 5
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 
convolution bias_data == NULL
the outptr distance of outptr and odata is 5 
the output data is 
0.729571 0.963435 20187612156465402972844524826911571968.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         5x2x1x1         1x5x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af1f630>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af19270>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1705[label="feed"]
   node_1704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1711[label="fetch"]
   node_1710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1708[label="filter_data"]
   node_1706[label="input_data"]
   node_1709[label="output_data"]
   node_1705->node_1704[label="X"]
   node_1704->node_1706[label="Out"]
   node_1708->node_1707[label="Filter"]
   node_1706->node_1707[label="Input"]
   node_1707->node_1709[label="Output"]
   node_1709->node_1710[label="X"]
   node_1710->node_1711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1745[label="feed"]
   node_1744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1751[label="fetch"]
   node_1750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1748[label="filter_data"]
   node_1746[label="input_data"]
   node_1749[label="output_data"]
   node_1745->node_1744[label="X"]
   node_1744->node_1746[label="Out"]
   node_1748->node_1747[label="Filter"]
   node_1746->node_1747[label="Input"]
   node_1747->node_1749[label="Output"]
   node_1749->node_1750[label="X"]
   node_1750->node_1751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea59b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 5
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 
convolution bias_data == NULL
the outptr distance of outptr and odata is 5 
the output data is 
0.729571 0.963435 0.000000 0.000063 2419606691581754018735681986394849280.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         5x2x1x1         1x5x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (3, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (4, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 50, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (50, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 50, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (18, 4, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (15, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (16, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (9, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (7, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (7, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aeaca70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aeac830>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1785[label="feed"]
   node_1784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1791[label="fetch"]
   node_1790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1788[label="filter_data"]
   node_1786[label="input_data"]
   node_1789[label="output_data"]
   node_1785->node_1784[label="X"]
   node_1784->node_1786[label="Out"]
   node_1788->node_1787[label="Filter"]
   node_1786->node_1787[label="Input"]
   node_1787->node_1789[label="Output"]
   node_1789->node_1790[label="X"]
   node_1790->node_1791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1825[label="feed"]
   node_1824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1831[label="fetch"]
   node_1830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1828[label="filter_data"]
   node_1826[label="input_data"]
   node_1829[label="output_data"]
   node_1825->node_1824[label="X"]
   node_1824->node_1826[label="Out"]
   node_1828->node_1827[label="Filter"]
   node_1826->node_1827[label="Input"]
   node_1827->node_1829[label="Output"]
   node_1829->node_1830[label="X"]
   node_1830->node_1831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea7330>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 7
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 7 
the output data is 
0.729571 0.963435 0.000000 0.000063 2419606691581754018735681986394849280.000000 0.000000 25213818898057127528732120108756893696.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         7x2x1x1         1x7x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (7, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aea7270>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aeaca70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1865[label="feed"]
   node_1864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1871[label="fetch"]
   node_1870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1868[label="filter_data"]
   node_1866[label="input_data"]
   node_1869[label="output_data"]
   node_1865->node_1864[label="X"]
   node_1864->node_1866[label="Out"]
   node_1868->node_1867[label="Filter"]
   node_1866->node_1867[label="Input"]
   node_1867->node_1869[label="Output"]
   node_1869->node_1870[label="X"]
   node_1870->node_1871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1905[label="feed"]
   node_1904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1911[label="fetch"]
   node_1910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1908[label="filter_data"]
   node_1906[label="input_data"]
   node_1909[label="output_data"]
   node_1905->node_1904[label="X"]
   node_1904->node_1906[label="Out"]
   node_1908->node_1907[label="Filter"]
   node_1906->node_1907[label="Input"]
   node_1907->node_1909[label="Output"]
   node_1909->node_1910[label="X"]
   node_1910->node_1911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea5170>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 7
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 7 
the output data is 
0.729571 0.963435 0.000000 0.000000 2677177183389366616064.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         7x2x1x1         1x7x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.6771771833893666e+21, index : 4.
base=[[[[0.8779403 ]]

  [[0.8031044 ]]

  [[0.8127694 ]]

  [[0.9737274 ]]

  [[0.10129949]]

  [[0.606577  ]]

  [[1.0492864 ]]]], 
arr=[[[[7.2957087e-01]]

  [[9.6343458e-01]]

  [[5.0306615e-43]]

  [[0.0000000e+00]]

  [[2.6771772e+21]]

  [[3.0594549e-41]]

  [[0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aeffb70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af14d70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1945[label="feed"]
   node_1944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1951[label="fetch"]
   node_1950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1948[label="filter_data"]
   node_1946[label="input_data"]
   node_1949[label="output_data"]
   node_1945->node_1944[label="X"]
   node_1944->node_1946[label="Out"]
   node_1948->node_1947[label="Filter"]
   node_1946->node_1947[label="Input"]
   node_1947->node_1949[label="Output"]
   node_1949->node_1950[label="X"]
   node_1950->node_1951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_1987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1985[label="feed"]
   node_1984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1991[label="fetch"]
   node_1990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_1988[label="filter_data"]
   node_1986[label="input_data"]
   node_1989[label="output_data"]
   node_1985->node_1984[label="X"]
   node_1984->node_1986[label="Out"]
   node_1988->node_1987[label="Filter"]
   node_1986->node_1987[label="Input"]
   node_1987->node_1989[label="Output"]
   node_1989->node_1990[label="X"]
   node_1990->node_1991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aef5d70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 6
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.729571 0.963435 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         6x2x1x1         1x6x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aef5fb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aeffb70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2025[label="feed"]
   node_2024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2031[label="fetch"]
   node_2030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2028[label="filter_data"]
   node_2026[label="input_data"]
   node_2029[label="output_data"]
   node_2025->node_2024[label="X"]
   node_2024->node_2026[label="Out"]
   node_2028->node_2027[label="Filter"]
   node_2026->node_2027[label="Input"]
   node_2027->node_2029[label="Output"]
   node_2029->node_2030[label="X"]
   node_2030->node_2031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2067[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2065[label="feed"]
   node_2064[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2071[label="fetch"]
   node_2070[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2068[label="filter_data"]
   node_2066[label="input_data"]
   node_2069[label="output_data"]
   node_2065->node_2064[label="X"]
   node_2064->node_2066[label="Out"]
   node_2068->node_2067[label="Filter"]
   node_2066->node_2067[label="Input"]
   node_2067->node_2069[label="Output"]
   node_2069->node_2070[label="X"]
   node_2070->node_2071[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187b1db870>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 6
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.729571 0.963435 0.000000 0.000063 2419606691581754018735681986394849280.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         6x2x1x1         1x6x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aec4370>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aec43f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2105[label="feed"]
   node_2104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2111[label="fetch"]
   node_2110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2108[label="filter_data"]
   node_2106[label="input_data"]
   node_2109[label="output_data"]
   node_2105->node_2104[label="X"]
   node_2104->node_2106[label="Out"]
   node_2108->node_2107[label="Filter"]
   node_2106->node_2107[label="Input"]
   node_2107->node_2109[label="Output"]
   node_2109->node_2110[label="X"]
   node_2110->node_2111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2145[label="feed"]
   node_2144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2151[label="fetch"]
   node_2150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2148[label="filter_data"]
   node_2146[label="input_data"]
   node_2149[label="output_data"]
   node_2145->node_2144[label="X"]
   node_2144->node_2146[label="Out"]
   node_2148->node_2147[label="Filter"]
   node_2146->node_2147[label="Input"]
   node_2147->node_2149[label="Output"]
   node_2149->node_2150[label="X"]
   node_2150->node_2151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aebb6b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.729571 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         1x2x1x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b227df0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af18bb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2185[label="feed"]
   node_2184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2191[label="fetch"]
   node_2190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2188[label="filter_data"]
   node_2186[label="input_data"]
   node_2189[label="output_data"]
   node_2185->node_2184[label="X"]
   node_2184->node_2186[label="Out"]
   node_2188->node_2187[label="Filter"]
   node_2186->node_2187[label="Input"]
   node_2187->node_2189[label="Output"]
   node_2189->node_2190[label="X"]
   node_2190->node_2191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2225[label="feed"]
   node_2224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2231[label="fetch"]
   node_2230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2228[label="filter_data"]
   node_2226[label="input_data"]
   node_2229[label="output_data"]
   node_2225->node_2224[label="X"]
   node_2224->node_2226[label="Out"]
   node_2228->node_2227[label="Filter"]
   node_2226->node_2227[label="Input"]
   node_2227->node_2229[label="Output"]
   node_2229->node_2230[label="X"]
   node_2230->node_2231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aec4d70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.729571 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         1x2x1x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af1feb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af1f930>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2265[label="feed"]
   node_2264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2271[label="fetch"]
   node_2270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2268[label="filter_data"]
   node_2266[label="input_data"]
   node_2269[label="output_data"]
   node_2265->node_2264[label="X"]
   node_2264->node_2266[label="Out"]
   node_2268->node_2267[label="Filter"]
   node_2266->node_2267[label="Input"]
   node_2267->node_2269[label="Output"]
   node_2269->node_2270[label="X"]
   node_2270->node_2271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2305[label="feed"]
   node_2304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2311[label="fetch"]
   node_2310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2308[label="filter_data"]
   node_2306[label="input_data"]
   node_2309[label="output_data"]
   node_2305->node_2304[label="X"]
   node_2304->node_2306[label="Out"]
   node_2308->node_2307[label="Filter"]
   node_2306->node_2307[label="Input"]
   node_2307->node_2309[label="Output"]
   node_2309->node_2310[label="X"]
   node_2310->node_2311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af20730>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 2
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
0.729571 0.963435 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         2x2x1x1         1x2x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af200b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af1feb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2345[label="feed"]
   node_2344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2351[label="fetch"]
   node_2350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2348[label="filter_data"]
   node_2346[label="input_data"]
   node_2349[label="output_data"]
   node_2345->node_2344[label="X"]
   node_2344->node_2346[label="Out"]
   node_2348->node_2347[label="Filter"]
   node_2346->node_2347[label="Input"]
   node_2347->node_2349[label="Output"]
   node_2349->node_2350[label="X"]
   node_2350->node_2351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2385[label="feed"]
   node_2384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2391[label="fetch"]
   node_2390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2388[label="filter_data"]
   node_2386[label="input_data"]
   node_2389[label="output_data"]
   node_2385->node_2384[label="X"]
   node_2384->node_2386[label="Out"]
   node_2388->node_2387[label="Filter"]
   node_2386->node_2387[label="Input"]
   node_2387->node_2389[label="Output"]
   node_2389->node_2390[label="X"]
   node_2390->node_2391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea7e30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 2
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
0.729571 0.963435 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         2x2x1x1         1x2x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (3, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aece2b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aece6f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,3,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2425[label="feed"]
   node_2424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2431[label="fetch"]
   node_2430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2428[label="filter_data"]
   node_2426[label="input_data"]
   node_2429[label="output_data"]
   node_2425->node_2424[label="X"]
   node_2424->node_2426[label="Out"]
   node_2428->node_2427[label="Filter"]
   node_2426->node_2427[label="Input"]
   node_2427->node_2429[label="Output"]
   node_2429->node_2430[label="X"]
   node_2430->node_2431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,3,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2465[label="feed"]
   node_2464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2471[label="fetch"]
   node_2470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2468[label="filter_data"]
   node_2466[label="input_data"]
   node_2469[label="output_data"]
   node_2465->node_2464[label="X"]
   node_2464->node_2466[label="Out"]
   node_2468->node_2467[label="Filter"]
   node_2466->node_2467[label="Input"]
   node_2467->node_2469[label="Output"]
   node_2469->node_2470[label="X"]
   node_2470->node_2471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aece430>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 3
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.729571 0.963435 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         3x2x1x1         1x3x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (3, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aece7b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aece2b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,3,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2505[label="feed"]
   node_2504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2511[label="fetch"]
   node_2510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2508[label="filter_data"]
   node_2506[label="input_data"]
   node_2509[label="output_data"]
   node_2505->node_2504[label="X"]
   node_2504->node_2506[label="Out"]
   node_2508->node_2507[label="Filter"]
   node_2506->node_2507[label="Input"]
   node_2507->node_2509[label="Output"]
   node_2509->node_2510[label="X"]
   node_2510->node_2511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,3,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2545[label="feed"]
   node_2544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2551[label="fetch"]
   node_2550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2548[label="filter_data"]
   node_2546[label="input_data"]
   node_2549[label="output_data"]
   node_2545->node_2544[label="X"]
   node_2544->node_2546[label="Out"]
   node_2548->node_2547[label="Filter"]
   node_2546->node_2547[label="Input"]
   node_2547->node_2549[label="Output"]
   node_2549->node_2550[label="X"]
   node_2550->node_2551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aed3bb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 3
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.729571 0.963435 20187612156465402972844524826911571968.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         3x2x1x1         1x3x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aece8f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aece330>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2585[label="feed"]
   node_2584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2591[label="fetch"]
   node_2590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2588[label="filter_data"]
   node_2586[label="input_data"]
   node_2589[label="output_data"]
   node_2585->node_2584[label="X"]
   node_2584->node_2586[label="Out"]
   node_2588->node_2587[label="Filter"]
   node_2586->node_2587[label="Input"]
   node_2587->node_2589[label="Output"]
   node_2589->node_2590[label="X"]
   node_2590->node_2591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2625[label="feed"]
   node_2624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2631[label="fetch"]
   node_2630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2628[label="filter_data"]
   node_2626[label="input_data"]
   node_2629[label="output_data"]
   node_2625->node_2624[label="X"]
   node_2624->node_2626[label="Out"]
   node_2628->node_2627[label="Filter"]
   node_2626->node_2627[label="Input"]
   node_2627->node_2629[label="Output"]
   node_2629->node_2630[label="X"]
   node_2630->node_2631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aecaef0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 
convolution bias_data == NULL
the outptr distance of outptr and odata is 4 
the output data is 
0.729571 0.963435 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         4x2x1x1         1x4x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aeca270>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aece8f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2665[label="feed"]
   node_2664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2671[label="fetch"]
   node_2670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2668[label="filter_data"]
   node_2666[label="input_data"]
   node_2669[label="output_data"]
   node_2665->node_2664[label="X"]
   node_2664->node_2666[label="Out"]
   node_2668->node_2667[label="Filter"]
   node_2666->node_2667[label="Input"]
   node_2667->node_2669[label="Output"]
   node_2669->node_2670[label="X"]
   node_2670->node_2671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2705[label="feed"]
   node_2704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2711[label="fetch"]
   node_2710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2708[label="filter_data"]
   node_2706[label="input_data"]
   node_2709[label="output_data"]
   node_2705->node_2704[label="X"]
   node_2704->node_2706[label="Out"]
   node_2708->node_2707[label="Filter"]
   node_2706->node_2707[label="Input"]
   node_2707->node_2709[label="Output"]
   node_2709->node_2710[label="X"]
   node_2710->node_2711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea5cb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ]],

        [[0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 2
the num is 1
the wout is 1
the hout is 1
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 
convolution bias_data == NULL
the outptr distance of outptr and odata is 4 
the output data is 
0.729571 0.963435 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x1x1         4x2x1x1         1x4x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (7, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1f1db0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1f14b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2745[label="feed"]
   node_2744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2751[label="fetch"]
   node_2750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2748[label="filter_data"]
   node_2746[label="input_data"]
   node_2749[label="output_data"]
   node_2745->node_2744[label="X"]
   node_2744->node_2746[label="Out"]
   node_2748->node_2747[label="Filter"]
   node_2746->node_2747[label="Input"]
   node_2747->node_2749[label="Output"]
   node_2749->node_2750[label="X"]
   node_2750->node_2751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2785[label="feed"]
   node_2784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2791[label="fetch"]
   node_2790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2788[label="filter_data"]
   node_2786[label="input_data"]
   node_2789[label="output_data"]
   node_2785->node_2784[label="X"]
   node_2784->node_2786[label="Out"]
   node_2788->node_2787[label="Filter"]
   node_2786->node_2787[label="Input"]
   node_2787->node_2789[label="Output"]
   node_2789->node_2790[label="X"]
   node_2790->node_2791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af197f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 7
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 7 
the output data is 
0.299039 0.000000 0.000000 0.000000 15094544860171758206976.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x1         7x1x1x1         1x7x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (7, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af190b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1f1db0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2825[label="feed"]
   node_2824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2831[label="fetch"]
   node_2830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2828[label="filter_data"]
   node_2826[label="input_data"]
   node_2829[label="output_data"]
   node_2825->node_2824[label="X"]
   node_2824->node_2826[label="Out"]
   node_2828->node_2827[label="Filter"]
   node_2826->node_2827[label="Input"]
   node_2827->node_2829[label="Output"]
   node_2829->node_2830[label="X"]
   node_2830->node_2831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,7,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2865[label="feed"]
   node_2864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2871[label="fetch"]
   node_2870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2868[label="filter_data"]
   node_2866[label="input_data"]
   node_2869[label="output_data"]
   node_2865->node_2864[label="X"]
   node_2864->node_2866[label="Out"]
   node_2868->node_2867[label="Filter"]
   node_2866->node_2867[label="Input"]
   node_2867->node_2869[label="Output"]
   node_2869->node_2870[label="X"]
   node_2870->node_2871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187a724770>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 1
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 7
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 
the weight data is 

0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 7 
the output data is 
0.299039 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x1         7x1x1x1         1x7x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 2, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187b1f1970>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b3202b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,2,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,2,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2905[label="feed"]
   node_2904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2911[label="fetch"]
   node_2910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2908[label="filter_data"]
   node_2906[label="input_data"]
   node_2909[label="output_data"]
   node_2905->node_2904[label="X"]
   node_2904->node_2906[label="Out"]
   node_2908->node_2907[label="Filter"]
   node_2906->node_2907[label="Input"]
   node_2907->node_2909[label="Output"]
   node_2909->node_2910[label="X"]
   node_2910->node_2911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,2,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,2,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2945[label="feed"]
   node_2944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2951[label="fetch"]
   node_2950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2948[label="filter_data"]
   node_2946[label="input_data"]
   node_2949[label="output_data"]
   node_2945->node_2944[label="X"]
   node_2944->node_2946[label="Out"]
   node_2948->node_2947[label="Filter"]
   node_2946->node_2947[label="Input"]
   node_2947->node_2949[label="Output"]
   node_2949->node_2950[label="X"]
   node_2950->node_2951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af18f30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934]],

        [[0.60276335],
         [0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 2
the chin is 2
the num is 1
the wout is 1
the hout is 2
the chout is 6
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 
convolution bias_data == NULL
the outptr distance of outptr and odata is 12 
the output data is 
0.819731 0.113118 0.514060 0.898059 27196353448380465531826480106585456640.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x2x1         6x2x1x1         1x6x2x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.7196353448380466e+37, index : 4.
base=[[[[0.8696663 ]
   [0.91060144]]

  [[0.0915039 ]
   [0.09827952]]

  [[0.51296884]
   [0.46814054]]

  [[0.9514744 ]
   [1.0305845 ]]

  [[1.0187825 ]
   [1.1353455 ]]

  [[0.7237405 ]
   [0.75534236]]]], 
arr=[[[[8.1973052e-01]
   [1.1311818e-01]]

  [[5.1405960e-01]
   [8.9805949e-01]]

  [[2.7196353e+37]
   [5.6626471e-41]]

  [[0.0000000e+00]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 3, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aea5870>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aea5fb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,3,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,3,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_2987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2985[label="feed"]
   node_2984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2991[label="fetch"]
   node_2990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_2988[label="filter_data"]
   node_2986[label="input_data"]
   node_2989[label="output_data"]
   node_2985->node_2984[label="X"]
   node_2984->node_2986[label="Out"]
   node_2988->node_2987[label="Filter"]
   node_2986->node_2987[label="Input"]
   node_2987->node_2989[label="Output"]
   node_2989->node_2990[label="X"]
   node_2990->node_2991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,3,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,3,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3025[label="feed"]
   node_3024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3031[label="fetch"]
   node_3030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3028[label="filter_data"]
   node_3026[label="input_data"]
   node_3029[label="output_data"]
   node_3025->node_3024[label="X"]
   node_3024->node_3026[label="Out"]
   node_3028->node_3027[label="Filter"]
   node_3026->node_3027[label="Input"]
   node_3027->node_3029[label="Output"]
   node_3029->node_3030[label="X"]
   node_3030->node_3031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aeacab0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934],
         [0.60276335]],

        [[0.5448832 ],
         [0.4236548 ],
         [0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 3
the chin is 2
the num is 1
the wout is 1
the hout is 3
the chout is 5
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 0.143353 0.944669 
convolution bias_data == NULL
the outptr distance of outptr and odata is 15 
the output data is 
0.904537 1.271447 0.748637 0.413128 0.460946 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x3x1         5x2x1x1         1x5x3x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.1060471534729004, index : 5.
base=[[[[0.901118  ]
   [0.9251142 ]
   [1.0309801 ]]

  [[0.972527  ]
   [1.0384648 ]
   [1.1060472 ]]

  [[0.67856336]
   [0.66072005]
   [0.782302  ]]

  [[0.41359282]
   [0.3556942 ]
   [0.4846127 ]]

  [[0.5934084 ]
   [0.50273824]
   [0.6965642 ]]]], 
arr=[[[[0.90453744]
   [1.2714472 ]
   [0.7486372 ]]

  [[0.41312796]
   [0.4609458 ]
   [0.        ]]

  [[0.        ]
   [0.        ]
   [0.        ]]

  [[0.        ]
   [0.        ]
   [0.        ]]

  [[0.        ]
   [0.        ]
   [0.        ]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 4, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aecaab0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aecae30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,4,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,4,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3067[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3065[label="feed"]
   node_3064[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3071[label="fetch"]
   node_3070[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3068[label="filter_data"]
   node_3066[label="input_data"]
   node_3069[label="output_data"]
   node_3065->node_3064[label="X"]
   node_3064->node_3066[label="Out"]
   node_3068->node_3067[label="Filter"]
   node_3066->node_3067[label="Input"]
   node_3067->node_3069[label="Output"]
   node_3069->node_3070[label="X"]
   node_3070->node_3071[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,4,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,4,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3105[label="feed"]
   node_3104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3111[label="fetch"]
   node_3110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3108[label="filter_data"]
   node_3106[label="input_data"]
   node_3109[label="output_data"]
   node_3105->node_3104[label="X"]
   node_3104->node_3106[label="Out"]
   node_3108->node_3107[label="Filter"]
   node_3106->node_3107[label="Input"]
   node_3107->node_3109[label="Output"]
   node_3109->node_3110[label="X"]
   node_3110->node_3111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aeace70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934],
         [0.60276335],
         [0.5448832 ]],

        [[0.4236548 ],
         [0.6458941 ],
         [0.4375872 ],
         [0.891773  ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 4
the chin is 2
the num is 1
the wout is 1
the hout is 4
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 
the weight data is 

0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
convolution bias_data == NULL
the outptr distance of outptr and odata is 16 
the output data is 
0.416108 0.778142 0.564494 0.566019 0.000000 0.000000 1936616200488264883952179740672.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x4x1         4x2x1x1         1x4x4x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 4, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af20670>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aecaab0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,4,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,4,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3145[label="feed"]
   node_3144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3151[label="fetch"]
   node_3150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3148[label="filter_data"]
   node_3146[label="input_data"]
   node_3149[label="output_data"]
   node_3145->node_3144[label="X"]
   node_3144->node_3146[label="Out"]
   node_3148->node_3147[label="Filter"]
   node_3146->node_3147[label="Input"]
   node_3147->node_3149[label="Output"]
   node_3149->node_3150[label="X"]
   node_3150->node_3151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,4,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,4,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3185[label="feed"]
   node_3184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3191[label="fetch"]
   node_3190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3188[label="filter_data"]
   node_3186[label="input_data"]
   node_3189[label="output_data"]
   node_3185->node_3184[label="X"]
   node_3184->node_3186[label="Out"]
   node_3188->node_3187[label="Filter"]
   node_3186->node_3187[label="Input"]
   node_3187->node_3189[label="Output"]
   node_3189->node_3190[label="X"]
   node_3190->node_3191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aeced30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934],
         [0.60276335],
         [0.5448832 ]],

        [[0.4236548 ],
         [0.6458941 ],
         [0.4375872 ],
         [0.891773  ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 4
the chin is 2
the num is 1
the wout is 1
the hout is 4
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 
the weight data is 

0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
convolution bias_data == NULL
the outptr distance of outptr and odata is 16 
the output data is 
0.416108 0.778142 0.564494 0.566019 1874955996868912885696819101696.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000014 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x2x4x1         4x2x1x1         1x4x4x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aebbab0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aebb7b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,3,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3225[label="feed"]
   node_3224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3231[label="fetch"]
   node_3230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3228[label="filter_data"]
   node_3226[label="input_data"]
   node_3229[label="output_data"]
   node_3225->node_3224[label="X"]
   node_3224->node_3226[label="Out"]
   node_3228->node_3227[label="Filter"]
   node_3226->node_3227[label="Input"]
   node_3227->node_3229[label="Output"]
   node_3229->node_3230[label="X"]
   node_3230->node_3231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,3,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3265[label="feed"]
   node_3264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3271[label="fetch"]
   node_3270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3268[label="filter_data"]
   node_3266[label="input_data"]
   node_3269[label="output_data"]
   node_3265->node_3264[label="X"]
   node_3264->node_3266[label="Out"]
   node_3268->node_3267[label="Filter"]
   node_3266->node_3267[label="Input"]
   node_3267->node_3269[label="Output"]
   node_3269->node_3270[label="X"]
   node_3270->node_3271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea7730>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934],
         [0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 3
the chin is 1
the num is 1
the wout is 1
the hout is 3
the chout is 6
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 
the weight data is 

0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 
convolution bias_data == NULL
the outptr distance of outptr and odata is 18 
the output data is 
0.210438 0.566233 0.318798 0.000000 0.000000 0.000000 0.000000 0.000000 inf 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x1         6x1x1x1         1x6x3x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : inf, index : 8.
base=[[[[0.21043788]
   [0.27423328]
   [0.23112449]]

  [[0.4345094 ]
   [0.5662333 ]
   [0.47722283]]

  [[0.29026467]
   [0.37826   ]
   [0.31879845]]

  [[0.31175053]
   [0.4062594 ]
   [0.34239644]]

  [[0.50798   ]
   [0.6619769 ]
   [0.55791575]]

  [[0.03898555]
   [0.05080423]
   [0.04281793]]]], 
arr=[[[[2.1043788e-01]
   [5.6623328e-01]
   [3.1879845e-01]]

  [[2.5898798e-41]
   [2.7295140e-36]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]
   [          inf]]

  [[0.0000000e+00]
   [0.0000000e+00]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]
   [0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae64770>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae64b30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3305[label="feed"]
   node_3304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3311[label="fetch"]
   node_3310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3308[label="filter_data"]
   node_3306[label="input_data"]
   node_3309[label="output_data"]
   node_3305->node_3304[label="X"]
   node_3304->node_3306[label="Out"]
   node_3308->node_3307[label="Filter"]
   node_3306->node_3307[label="Input"]
   node_3307->node_3309[label="Output"]
   node_3309->node_3310[label="X"]
   node_3310->node_3311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3345[label="feed"]
   node_3344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3351[label="fetch"]
   node_3350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3348[label="filter_data"]
   node_3346[label="input_data"]
   node_3349[label="output_data"]
   node_3345->node_3344[label="X"]
   node_3344->node_3346[label="Out"]
   node_3348->node_3347[label="Filter"]
   node_3346->node_3347[label="Input"]
   node_3347->node_3349[label="Output"]
   node_3349->node_3350[label="X"]
   node_3350->node_3351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae6dd30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 2
the hout is 2
the chout is 6
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 
convolution bias_data == NULL
the outptr distance of outptr and odata is 24 
the output data is 
0.311751 0.661977 0.050804 0.052518 309300150711158661459090919577878528.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 inf 0.000000 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x2         6x1x1x1         1x6x2x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : inf, index : 12.
base=[[[[0.31175053 0.4062594 ]
   [0.34239644 0.30951792]]

  [[0.50798    0.6619769 ]
   [0.55791575 0.5043421 ]]

  [[0.03898555 0.05080423]
   [0.04281793 0.03870635]]

  [[0.04781774 0.06231395]
   [0.05251835 0.04747529]]

  [[0.01109613 0.01445998]
   [0.01218691 0.01101666]]

  [[0.45695302 0.59548086]
   [0.5018727  0.45368055]]]], 
arr=[[[[3.1175053e-01 6.6197687e-01]
   [5.0804231e-02 5.2518349e-02]]

  [[3.0930015e+35 3.7961175e-41]
   [3.2354328e-18 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]

  [[          inf 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aec0e70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aec0cf0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3385[label="feed"]
   node_3384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3391[label="fetch"]
   node_3390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3388[label="filter_data"]
   node_3386[label="input_data"]
   node_3389[label="output_data"]
   node_3385->node_3384[label="X"]
   node_3384->node_3386[label="Out"]
   node_3388->node_3387[label="Filter"]
   node_3386->node_3387[label="Input"]
   node_3387->node_3389[label="Output"]
   node_3389->node_3390[label="X"]
   node_3390->node_3391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,6,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3425[label="feed"]
   node_3424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3431[label="fetch"]
   node_3430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3428[label="filter_data"]
   node_3426[label="input_data"]
   node_3429[label="output_data"]
   node_3425->node_3424[label="X"]
   node_3424->node_3426[label="Out"]
   node_3428->node_3427[label="Filter"]
   node_3426->node_3427[label="Input"]
   node_3427->node_3429[label="Output"]
   node_3429->node_3430[label="X"]
   node_3430->node_3431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea74f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 1
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 6
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 
the weight data is 

0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 
convolution bias_data == NULL
the outptr distance of outptr and odata is 18 
the output data is 
0.210438 0.566233 0.318798 0.000000 15667059333301698297856.000000 0.000000 0.000000 0.000000 inf 0.000000 47.696228 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x3         6x1x1x1         1x6x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : inf, index : 8.
base=[[[[0.21043788 0.27423328 0.23112449]]

  [[0.4345094  0.5662333  0.47722283]]

  [[0.29026467 0.37826    0.31879845]]

  [[0.31175053 0.4062594  0.34239644]]

  [[0.50798    0.6619769  0.55791575]]

  [[0.03898555 0.05080423 0.04281793]]]], 
arr=[[[[2.1043788e-01 5.6623328e-01 3.1879845e-01]]

  [[0.0000000e+00 1.5667059e+22 2.5489619e-42]]

  [[0.0000000e+00 0.0000000e+00           inf]]

  [[0.0000000e+00 4.7696228e+01 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00 0.0000000e+00]]

  [[0.0000000e+00 4.0322298e-22 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af1f5f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187b1f1430>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3465[label="feed"]
   node_3464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3471[label="fetch"]
   node_3470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3468[label="filter_data"]
   node_3466[label="input_data"]
   node_3469[label="output_data"]
   node_3465->node_3464[label="X"]
   node_3464->node_3466[label="Out"]
   node_3468->node_3467[label="Filter"]
   node_3466->node_3467[label="Input"]
   node_3467->node_3469[label="Output"]
   node_3469->node_3470[label="X"]
   node_3470->node_3471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3505[label="feed"]
   node_3504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3511[label="fetch"]
   node_3510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3508[label="filter_data"]
   node_3506[label="input_data"]
   node_3509[label="output_data"]
   node_3505->node_3504[label="X"]
   node_3504->node_3506[label="Out"]
   node_3508->node_3507[label="Filter"]
   node_3506->node_3507[label="Input"]
   node_3507->node_3509[label="Output"]
   node_3509->node_3510[label="X"]
   node_3510->node_3511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aef5cb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 5
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 0.978618 0.799159 0.461479 
convolution bias_data == NULL
the outptr distance of outptr and odata is 30 
the output data is 
0.427063 0.622223 0.589875 0.571550 0.278163 0.000000 0.000000 0.000000 inf 0.000000 inf 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         5x1x1x1         1x5x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : inf, index : 8.
base=[[[[0.42706296 0.5565294  0.4690444 ]
   [0.42400452 0.32966983 0.50260687]]

  [[0.47747445 0.62222344 0.52441144]
   [0.474055   0.3685848  0.5619357 ]]

  [[0.537079   0.6998974  0.5898753 ]
   [0.5332327  0.41459635 0.63208383]]

  [[0.43858904 0.5715497  0.4817035 ]
   [0.43544808 0.33856735 0.5161718 ]]

  [[0.25326613 0.33004513 0.27816284]
   [0.25145236 0.19550794 0.2980668 ]]]], 
arr=[[[[4.2706296e-01 6.2222344e-01 5.8987528e-01]
   [5.7154971e-01 2.7816284e-01 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00           inf]
   [0.0000000e+00           inf 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 4.3411784e-14 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]

Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (3, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (7, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 2, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 3, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 2, 4, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 2, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (6, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aecaa70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aecaa30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3545[label="feed"]
   node_3544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3551[label="fetch"]
   node_3550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3548[label="filter_data"]
   node_3546[label="input_data"]
   node_3549[label="output_data"]
   node_3545->node_3544[label="X"]
   node_3544->node_3546[label="Out"]
   node_3548->node_3547[label="Filter"]
   node_3546->node_3547[label="Input"]
   node_3547->node_3549[label="Output"]
   node_3549->node_3550[label="X"]
   node_3550->node_3551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3585[label="feed"]
   node_3584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3591[label="fetch"]
   node_3590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3588[label="filter_data"]
   node_3586[label="input_data"]
   node_3589[label="output_data"]
   node_3585->node_3584[label="X"]
   node_3584->node_3586[label="Out"]
   node_3588->node_3587[label="Filter"]
   node_3586->node_3587[label="Input"]
   node_3587->node_3589[label="Output"]
   node_3589->node_3590[label="X"]
   node_3590->node_3591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aeac870>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 0.414662 0.264556 
convolution bias_data == NULL
the outptr distance of outptr and odata is 36 
the output data is 
0.518447 0.373220 0.249943 0.189207 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x3         4x1x1x1         1x4x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aeac030>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aeffcb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3625[label="feed"]
   node_3624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3631[label="fetch"]
   node_3630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3628[label="filter_data"]
   node_3626[label="input_data"]
   node_3629[label="output_data"]
   node_3625->node_3624[label="X"]
   node_3624->node_3626[label="Out"]
   node_3628->node_3627[label="Filter"]
   node_3626->node_3627[label="Input"]
   node_3627->node_3629[label="Output"]
   node_3629->node_3630[label="X"]
   node_3630->node_3631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3665[label="feed"]
   node_3664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3671[label="fetch"]
   node_3670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3668[label="filter_data"]
   node_3666[label="input_data"]
   node_3669[label="output_data"]
   node_3665->node_3664[label="X"]
   node_3664->node_3666[label="Out"]
   node_3668->node_3667[label="Filter"]
   node_3666->node_3667[label="Input"]
   node_3667->node_3669[label="Output"]
   node_3669->node_3670[label="X"]
   node_3670->node_3671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af183f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 4
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 0.414662 0.264556 
convolution bias_data == NULL
the outptr distance of outptr and odata is 36 
the output data is 
0.518447 0.373220 0.249943 0.189207 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x3         4x1x1x1         1x4x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aec0530>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aec00f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,2,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3705[label="feed"]
   node_3704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3711[label="fetch"]
   node_3710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3708[label="filter_data"]
   node_3706[label="input_data"]
   node_3709[label="output_data"]
   node_3705->node_3704[label="X"]
   node_3704->node_3706[label="Out"]
   node_3708->node_3707[label="Filter"]
   node_3706->node_3707[label="Input"]
   node_3707->node_3709[label="Output"]
   node_3709->node_3710[label="X"]
   node_3710->node_3711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,2,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3745[label="feed"]
   node_3744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3751[label="fetch"]
   node_3750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3748[label="filter_data"]
   node_3746[label="input_data"]
   node_3749[label="output_data"]
   node_3745->node_3744[label="X"]
   node_3744->node_3746[label="Out"]
   node_3748->node_3747[label="Filter"]
   node_3746->node_3747[label="Input"]
   node_3747->node_3749[label="Output"]
   node_3749->node_3750[label="X"]
   node_3750->node_3751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae7b030>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 1
the hout is 2
the chout is 5
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 0.870012 0.978618 0.799159 
convolution bias_data == NULL
the outptr distance of outptr and odata is 10 
the output data is 
0.973727 0.103323 0.000000 0.000000 16564527659844835999744.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x2x2         5x1x1x2         1x5x2x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.6564527659844836e+22, index : 4.
base=[[[[0.9737274 ]
   [0.8467385 ]]

  [[0.10129949]
   [0.09029322]]

  [[0.606577  ]
   [0.46586746]]

  [[1.0492864 ]
   [0.9430994 ]]

  [[1.1086286 ]
   [1.0253234 ]]]], 
arr=[[[[9.7372741e-01]
   [1.0332258e-01]]

  [[6.5131422e-38]
   [4.2038954e-45]]

  [[1.6564528e+22]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]]

  [[0.0000000e+00]
   [0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae818f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae81470>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3785[label="feed"]
   node_3784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3791[label="fetch"]
   node_3790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3788[label="filter_data"]
   node_3786[label="input_data"]
   node_3789[label="output_data"]
   node_3785->node_3784[label="X"]
   node_3784->node_3786[label="Out"]
   node_3788->node_3787[label="Filter"]
   node_3786->node_3787[label="Input"]
   node_3787->node_3789[label="Output"]
   node_3789->node_3790[label="X"]
   node_3790->node_3791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,5,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3825[label="feed"]
   node_3824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3831[label="fetch"]
   node_3830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3828[label="filter_data"]
   node_3826[label="input_data"]
   node_3829[label="output_data"]
   node_3825->node_3824[label="X"]
   node_3824->node_3826[label="Out"]
   node_3828->node_3827[label="Filter"]
   node_3826->node_3827[label="Input"]
   node_3827->node_3829[label="Output"]
   node_3829->node_3830[label="X"]
   node_3830->node_3831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae83e30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 1
the chin is 1
the num is 1
the wout is 2
the hout is 1
the chout is 5
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 
the weight data is 

0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 0.087129 0.020218 0.832620 0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 10 
the output data is 
0.776671 0.720656 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x1x3         5x1x1x2         1x5x1x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.0645252466201782, index : 9.
base=[[[[0.7766712  0.75145614]]

  [[0.6965241  0.7206564 ]]

  [[0.55878425 0.7047948 ]]

  [[0.06227772 0.07450086]]

  [[1.0134825  1.0645252 ]]]], 
arr=[[[[7.766712e-01 7.206564e-01]]

  [[2.070880e-37 2.539521e-37]]

  [[4.203895e-45 0.000000e+00]]

  [[0.000000e+00 0.000000e+00]]

  [[0.000000e+00 0.000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aebbeb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aebb8f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3865[label="feed"]
   node_3864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3871[label="fetch"]
   node_3870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3868[label="filter_data"]
   node_3866[label="input_data"]
   node_3869[label="output_data"]
   node_3865->node_3864[label="X"]
   node_3864->node_3866[label="Out"]
   node_3868->node_3867[label="Filter"]
   node_3866->node_3867[label="Input"]
   node_3867->node_3869[label="Output"]
   node_3869->node_3870[label="X"]
   node_3870->node_3871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,4,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3905[label="feed"]
   node_3904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3911[label="fetch"]
   node_3910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3908[label="filter_data"]
   node_3906[label="input_data"]
   node_3909[label="output_data"]
   node_3905->node_3904[label="X"]
   node_3904->node_3906[label="Out"]
   node_3908->node_3907[label="Filter"]
   node_3906->node_3907[label="Input"]
   node_3907->node_3909[label="Output"]
   node_3909->node_3910[label="X"]
   node_3910->node_3911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af3b7f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 2
the hout is 2
the chout is 4
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 0.978618 0.799159 0.461479 0.780529 0.118274 0.639921 
convolution bias_data == NULL
the outptr distance of outptr and odata is 16 
the output data is 
1.049286 1.181601 0.800520 0.419974 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x2x3         4x1x1x2         1x4x2x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.1816009283065796, index : 5.
base=[[[[1.0492864  1.0809408 ]
   [0.79258937 0.89160556]]

  [[1.1086286  1.1816009 ]
   [0.87180007 0.9307682 ]]

  [[0.81149226 0.8005195 ]
   [0.5821273  0.6996471 ]]

  [[0.52257526 0.47030956]
   [0.33555135 0.46342874]]]], 
arr=[[[[1.0492864e+00 1.1816009e+00]
   [8.0051953e-01 4.1997367e-01]]

  [[3.5189046e-37 1.5414283e-44]
   [1.5414283e-44 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [3.9381963e-25 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (3, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae855f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae85a30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,3,3,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3945[label="feed"]
   node_3944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3951[label="fetch"]
   node_3950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3948[label="filter_data"]
   node_3946[label="input_data"]
   node_3949[label="output_data"]
   node_3945->node_3944[label="X"]
   node_3944->node_3946[label="Out"]
   node_3948->node_3947[label="Filter"]
   node_3946->node_3947[label="Input"]
   node_3947->node_3949[label="Output"]
   node_3949->node_3950[label="X"]
   node_3950->node_3951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,3,3,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_3987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3985[label="feed"]
   node_3984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3991[label="fetch"]
   node_3990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_3988[label="filter_data"]
   node_3986[label="input_data"]
   node_3989[label="output_data"]
   node_3985->node_3984[label="X"]
   node_3984->node_3986[label="Out"]
   node_3988->node_3987[label="Filter"]
   node_3986->node_3987[label="Input"]
   node_3987->node_3989[label="Output"]
   node_3989->node_3990[label="X"]
   node_3990->node_3991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae90c30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 2
the hout is 3
the chout is 3
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 0.414662 0.264556 0.774234 0.456150 
convolution bias_data == NULL
the outptr distance of outptr and odata is 18 
the output data is 
0.891667 0.456026 0.828674 0.000000 2667631127365840823428625465344.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x3x3         3x1x1x2         1x3x3x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.667631127365841e+30, index : 4.
base=[[[[0.8916674  0.99016815]
   [0.73581773 0.73727226]
   [0.87874526 1.3453159 ]]

  [[0.41677943 0.45602623]
   [0.3380226  0.3465484 ]
   [0.41737428 0.6247267 ]]

  [[0.75114375 0.8286744 ]
   [0.6151172  0.6226326 ]
   [0.7455773  1.1300157 ]]]], 
arr=[[[[8.9166737e-01 4.5602623e-01]
   [8.2867444e-01 5.6051939e-45]
   [2.6676311e+30 1.7241964e-36]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 4, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae90370>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aecaa30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,4,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,4,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4025[label="feed"]
   node_4024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4031[label="fetch"]
   node_4030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4028[label="filter_data"]
   node_4026[label="input_data"]
   node_4029[label="output_data"]
   node_4025->node_4024[label="X"]
   node_4024->node_4026[label="Out"]
   node_4028->node_4027[label="Filter"]
   node_4026->node_4027[label="Input"]
   node_4027->node_4029[label="Output"]
   node_4029->node_4030[label="X"]
   node_4030->node_4031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,4,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,4,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4067[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4065[label="feed"]
   node_4064[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4071[label="fetch"]
   node_4070[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4068[label="filter_data"]
   node_4066[label="input_data"]
   node_4069[label="output_data"]
   node_4065->node_4064[label="X"]
   node_4064->node_4066[label="Out"]
   node_4068->node_4067[label="Filter"]
   node_4066->node_4067[label="Input"]
   node_4067->node_4069[label="Output"]
   node_4069->node_4070[label="X"]
   node_4070->node_4071[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae859f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274],
         [0.3834415 , 0.79172504, 0.5288949 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 4
the chin is 1
the num is 1
the wout is 2
the hout is 4
the chout is 2
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 
the weight data is 

0.612096 0.616934 0.943748 0.681820 
convolution bias_data == NULL
the outptr distance of outptr and odata is 16 
the output data is 
0.777151 1.085935 0.000000 0.000000 0.000000 0.544883 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x4x3         2x1x1x2         1x2x4x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.498653769493103, index : 13.
base=[[[[0.77715105 0.8096295 ]
   [0.59488773 0.6577913 ]
   [0.81801033 1.1403667 ]
   [0.723145   0.81090474]]

  [[1.0055723  1.0859348 ]
   [0.8030889  0.8402071 ]
   [1.021001   1.4986538 ]
   [0.9016864  1.1078002 ]]]], 
arr=[[[[7.7715105e-01 1.0859348e+00]
   [1.4012985e-45 0.0000000e+00]
   [0.0000000e+00 5.4488319e-01]
   [0.0000000e+00 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 5, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae64f70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae812f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4105[label="feed"]
   node_4104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4111[label="fetch"]
   node_4110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4108[label="filter_data"]
   node_4106[label="input_data"]
   node_4109[label="output_data"]
   node_4105->node_4104[label="X"]
   node_4104->node_4106[label="Out"]
   node_4108->node_4107[label="Filter"]
   node_4106->node_4107[label="Input"]
   node_4107->node_4109[label="Output"]
   node_4109->node_4110[label="X"]
   node_4110->node_4111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4145[label="feed"]
   node_4144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4151[label="fetch"]
   node_4150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4148[label="filter_data"]
   node_4146[label="input_data"]
   node_4149[label="output_data"]
   node_4145->node_4144[label="X"]
   node_4144->node_4146[label="Out"]
   node_4148->node_4147[label="Filter"]
   node_4146->node_4147[label="Input"]
   node_4147->node_4149[label="Output"]
   node_4149->node_4150[label="X"]
   node_4150->node_4151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae7beb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274],
         [0.3834415 , 0.79172504, 0.5288949 ],
         [0.56804454, 0.92559665, 0.07103606]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 5
the chin is 1
the num is 1
the wout is 2
the hout is 5
the chout is 1
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 
the weight data is 

0.670638 0.210383 
convolution bias_data == NULL
the outptr distance of outptr and odata is 10 
the output data is 
0.518518 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.645894 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x5x3         1x1x1x2         1x1x5x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 5, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae7bf30>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae64f70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4185[label="feed"]
   node_4184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4191[label="fetch"]
   node_4190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4188[label="filter_data"]
   node_4186[label="input_data"]
   node_4189[label="output_data"]
   node_4185->node_4184[label="X"]
   node_4184->node_4186[label="Out"]
   node_4188->node_4187[label="Filter"]
   node_4186->node_4187[label="Input"]
   node_4187->node_4189[label="Output"]
   node_4189->node_4190[label="X"]
   node_4190->node_4191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,5,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4225[label="feed"]
   node_4224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4231[label="fetch"]
   node_4230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4228[label="filter_data"]
   node_4226[label="input_data"]
   node_4229[label="output_data"]
   node_4225->node_4224[label="X"]
   node_4224->node_4226[label="Out"]
   node_4228->node_4227[label="Filter"]
   node_4226->node_4227[label="Input"]
   node_4227->node_4229[label="Output"]
   node_4229->node_4230[label="X"]
   node_4230->node_4231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae98c30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274],
         [0.3834415 , 0.79172504, 0.5288949 ],
         [0.56804454, 0.92559665, 0.07103606]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 5
the chin is 1
the num is 1
the wout is 2
the hout is 5
the chout is 1
the kw is 2
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 0.568045 0.925597 0.071036 
the weight data is 

0.670638 0.210383 
convolution bias_data == NULL
the outptr distance of outptr and odata is 10 
the output data is 
0.518518 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x2p0s1g1d1unk             1x1x5x3         1x1x1x2         1x1x5x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 4, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae9d2b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae9d6b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,4,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4265[label="feed"]
   node_4264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4271[label="fetch"]
   node_4270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4268[label="filter_data"]
   node_4266[label="input_data"]
   node_4269[label="output_data"]
   node_4265->node_4264[label="X"]
   node_4264->node_4266[label="Out"]
   node_4268->node_4267[label="Filter"]
   node_4266->node_4267[label="Input"]
   node_4267->node_4269[label="Output"]
   node_4269->node_4270[label="X"]
   node_4270->node_4271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,4,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4305[label="feed"]
   node_4304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4311[label="fetch"]
   node_4310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4308[label="filter_data"]
   node_4306[label="input_data"]
   node_4309[label="output_data"]
   node_4305->node_4304[label="X"]
   node_4304->node_4306[label="Out"]
   node_4308->node_4307[label="Filter"]
   node_4306->node_4307[label="Input"]
   node_4307->node_4309[label="Output"]
   node_4309->node_4310[label="X"]
   node_4310->node_4311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae98030>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274],
         [0.3834415 , 0.79172504, 0.5288949 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 4
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 2
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 
the weight data is 

0.612096 0.616934 0.943748 0.681820 
convolution bias_data == NULL
the outptr distance of outptr and odata is 18 
the output data is 
0.672083 0.963815 0.000000 0.000000 0.000000 0.000000 0.000000 0.544883 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x4x3         2x1x2x1         1x2x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.3814232349395752, index : 16.
base=[[[[0.6720834  0.69913137 0.7674229 ]
   [0.6034831  0.80948234 0.98986536]
   [0.50440335 1.0342925  0.91614705]]

  [[0.8894541  0.963815   1.0092404 ]
   [0.8125883  1.0078523  1.2666061 ]
   [0.6744103  1.3814232  1.2700661 ]]]], 
arr=[[[[6.7208338e-01 9.6381497e-01 3.3696431e-11]
   [2.2138042e-36 1.4012985e-45 0.0000000e+00]
   [0.0000000e+00 5.4488319e-01 0.0000000e+00]]

  [[0.0000000e+00 0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 4, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aea5eb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aea5930>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,4,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,3,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4345[label="feed"]
   node_4344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4351[label="fetch"]
   node_4350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4348[label="filter_data"]
   node_4346[label="input_data"]
   node_4349[label="output_data"]
   node_4345->node_4344[label="X"]
   node_4344->node_4346[label="Out"]
   node_4348->node_4347[label="Filter"]
   node_4346->node_4347[label="Input"]
   node_4347->node_4349[label="Output"]
   node_4349->node_4350[label="X"]
   node_4350->node_4351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,4,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,3,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4385[label="feed"]
   node_4384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4391[label="fetch"]
   node_4390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4388[label="filter_data"]
   node_4386[label="input_data"]
   node_4389[label="output_data"]
   node_4385->node_4384[label="X"]
   node_4384->node_4386[label="Out"]
   node_4388->node_4387[label="Filter"]
   node_4386->node_4387[label="Input"]
   node_4387->node_4389[label="Output"]
   node_4389->node_4390[label="X"]
   node_4390->node_4391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aec05b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 4
the chin is 1
the num is 1
the wout is 1
the hout is 3
the chout is 2
the kw is 2
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 
the weight data is 

0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
1.123718 1.095323 2667631127365840823428625465344.000000 0.000013 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x2p0s1g1d1unk             1x1x4x2         2x1x2x2         1x2x3x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.667631127365841e+30, index : 2.
base=[[[[1.1237175]
   [1.090862 ]
   [1.3685884]]

  [[1.1642905]
   [1.1526467]
   [1.2951181]]]], 
arr=[[[[1.1237175e+00]
   [1.0953231e+00]
   [2.6676311e+30]]

  [[1.3167265e-05]
   [0.0000000e+00]
   [0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae2c4f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae2c8b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4425[label="feed"]
   node_4424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4431[label="fetch"]
   node_4430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4428[label="filter_data"]
   node_4426[label="input_data"]
   node_4429[label="output_data"]
   node_4425->node_4424[label="X"]
   node_4424->node_4426[label="Out"]
   node_4428->node_4427[label="Filter"]
   node_4426->node_4427[label="Input"]
   node_4427->node_4429[label="Output"]
   node_4429->node_4430[label="X"]
   node_4430->node_4431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4465[label="feed"]
   node_4464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4471[label="fetch"]
   node_4470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4468[label="filter_data"]
   node_4466[label="input_data"]
   node_4469[label="output_data"]
   node_4465->node_4464[label="X"]
   node_4464->node_4466[label="Out"]
   node_4468->node_4467[label="Filter"]
   node_4466->node_4467[label="Input"]
   node_4467->node_4469[label="Output"]
   node_4469->node_4470[label="X"]
   node_4470->node_4471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae2cdb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 2
the hout is 2
the chout is 2
the kw is 2
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 0.414662 0.264556 0.774234 0.456150 0.568434 0.018790 
convolution bias_data == NULL
the outptr distance of outptr and odata is 8 
the output data is 
1.229690 1.081630 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x2p0s1g1d1unk             1x1x3x3         2x1x2x2         1x2x2x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.3619990348815918, index : 3.
base=[[[[1.22969    1.3367167 ]
   [1.153192   1.361999  ]]

  [[1.0688343  1.0816303 ]
   [0.88061285 1.1476537 ]]]], 
arr=[[[[1.2296900e+00 1.0816305e+00]
   [2.1237511e-36 1.4012985e-44]]

  [[0.0000000e+00 0.0000000e+00]
   [0.0000000e+00 0.0000000e+00]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 4), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aec0cb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae64070>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,4}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4505[label="feed"]
   node_4504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4511[label="fetch"]
   node_4510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4508[label="filter_data"]
   node_4506[label="input_data"]
   node_4509[label="output_data"]
   node_4505->node_4504[label="X"]
   node_4504->node_4506[label="Out"]
   node_4508->node_4507[label="Filter"]
   node_4506->node_4507[label="Input"]
   node_4507->node_4509[label="Output"]
   node_4509->node_4510[label="X"]
   node_4510->node_4511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,4}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,2,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4545[label="feed"]
   node_4544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4551[label="fetch"]
   node_4550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4548[label="filter_data"]
   node_4546[label="input_data"]
   node_4549[label="output_data"]
   node_4545->node_4544[label="X"]
   node_4544->node_4546[label="Out"]
   node_4548->node_4547[label="Filter"]
   node_4546->node_4547[label="Input"]
   node_4547->node_4549[label="Output"]
   node_4549->node_4550[label="X"]
   node_4550->node_4551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae7b9f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 , 0.4375872 , 0.891773  ]]]],
      dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 4
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 2
the kw is 2
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 
the weight data is 

0.118274 0.639921 0.143353 0.944669 0.521848 0.414662 0.264556 0.774234 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
1.193464 1.132833 9914816517141521294201448300544.000000 0.000000 inf 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x2p0s1g1d1unk             1x1x2x4         2x1x2x2         1x2x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : inf, index : 4.
base=[[[[1.1934637 0.9762756 1.3251334]]

  [[1.1951125 1.132833  1.3467002]]]], 
arr=[[[[1.1934636e+00 1.1328330e+00 9.9148165e+30]]

  [[5.5819914e-08           inf 2.2690140e-13]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 4), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aefff30>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aeff530>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,4}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4585[label="feed"]
   node_4584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4591[label="fetch"]
   node_4590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4588[label="filter_data"]
   node_4586[label="input_data"]
   node_4589[label="output_data"]
   node_4585->node_4584[label="X"]
   node_4584->node_4586[label="Out"]
   node_4588->node_4587[label="Filter"]
   node_4586->node_4587[label="Input"]
   node_4587->node_4589[label="Output"]
   node_4589->node_4590[label="X"]
   node_4590->node_4591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,4}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4625[label="feed"]
   node_4624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4631[label="fetch"]
   node_4630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4628[label="filter_data"]
   node_4626[label="input_data"]
   node_4629[label="output_data"]
   node_4625->node_4624[label="X"]
   node_4624->node_4626[label="Out"]
   node_4628->node_4627[label="Filter"]
   node_4626->node_4627[label="Input"]
   node_4627->node_4629[label="Output"]
   node_4629->node_4630[label="X"]
   node_4630->node_4631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea7af0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 , 0.4375872 , 0.891773  ],
         [0.96366274, 0.3834415 , 0.79172504, 0.5288949 ]]]],
      dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 4
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 2
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 
the weight data is 

0.612096 0.616934 0.943748 0.681820 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
1.617358 0.000000 0.000000 0.000000 0.000000 0.891773 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x2p0s1g1d1unk             1x1x3x4         1x1x2x2         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.8286844491958618, index : 3.
base=[[[[1.6173581 1.7175466 1.7261068]
   [1.8286844 1.5669979 1.9258106]]]], 
arr=[[[[1.6173581e+00 0.0000000e+00 1.3949378e-36]
   [1.9619603e-36 1.4012985e-45 8.9177299e-01]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (5, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (4, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (3, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 4, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 5, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 4, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 4, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 4), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (2, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 4), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 4), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 3, 1), 'lod': None, 'dtype': dtype('float32')}],
)py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 4), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 3, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae854f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae856b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,4}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,4}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4665[label="feed"]
   node_4664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4671[label="fetch"]
   node_4670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4668[label="filter_data"]
   node_4666[label="input_data"]
   node_4669[label="output_data"]
   node_4665->node_4664[label="X"]
   node_4664->node_4666[label="Out"]
   node_4668->node_4667[label="Filter"]
   node_4666->node_4667[label="Input"]
   node_4667->node_4669[label="Output"]
   node_4669->node_4670[label="X"]
   node_4670->node_4671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,4}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,4}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4705[label="feed"]
   node_4704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4711[label="fetch"]
   node_4710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4708[label="filter_data"]
   node_4706[label="input_data"]
   node_4709[label="output_data"]
   node_4705->node_4704[label="X"]
   node_4704->node_4706[label="Out"]
   node_4708->node_4707[label="Filter"]
   node_4706->node_4707[label="Input"]
   node_4707->node_4709[label="Output"]
   node_4709->node_4710[label="X"]
   node_4710->node_4711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae9d0b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 , 0.4375872 , 0.891773  ],
         [0.96366274, 0.3834415 , 0.79172504, 0.5288949 ]]]],
      dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 4
the hin is 3
the chin is 1
the num is 1
the wout is 4
the hout is 1
the chout is 1
the kw is 1
the kh is 3
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 0.383442 0.791725 0.528895 
the weight data is 

0.612096 0.616934 0.943748 
convolution bias_data == NULL
the outptr distance of outptr and odata is 4 
the output data is 
1.506748 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      3x1p0s1g1d1unk             1x1x3x4         1x1x3x1         1x1x1x4         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.3861002922058105, index : 2.
base=[[[[1.5067483 1.1981106 1.3861003 1.3828293]]]], 
arr=[[[[1.5067483e+00 0.0000000e+00 1.7297910e-36 2.7970977e-36]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae3c9f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae3cd70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4745[label="feed"]
   node_4744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4751[label="fetch"]
   node_4750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4748[label="filter_data"]
   node_4746[label="input_data"]
   node_4749[label="output_data"]
   node_4745->node_4744[label="X"]
   node_4744->node_4746[label="Out"]
   node_4748->node_4747[label="Filter"]
   node_4746->node_4747[label="Input"]
   node_4747->node_4749[label="Output"]
   node_4749->node_4750[label="X"]
   node_4750->node_4751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4785[label="feed"]
   node_4784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4791[label="fetch"]
   node_4790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4788[label="filter_data"]
   node_4786[label="input_data"]
   node_4789[label="output_data"]
   node_4785->node_4784[label="X"]
   node_4784->node_4786[label="Out"]
   node_4788->node_4787[label="Filter"]
   node_4786->node_4787[label="Input"]
   node_4787->node_4789[label="Output"]
   node_4789->node_4790[label="X"]
   node_4790->node_4791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae42fb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 2
the hout is 1
the chout is 1
the kw is 2
the kh is 3
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 0.414662 0.264556 0.774234 0.456150 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
1.975267 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      3x2p0s1g1d1unk             1x1x3x3         1x1x3x2         1x1x1x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 2.4667322635650635, index : 1.
base=[[[[1.9752672 2.4667323]]]], 
arr=[[[[1.9752673e+00 3.4167285e-36]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af18470>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae9d1b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4825[label="feed"]
   node_4824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4831[label="fetch"]
   node_4830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4828[label="filter_data"]
   node_4826[label="input_data"]
   node_4829[label="output_data"]
   node_4825->node_4824[label="X"]
   node_4824->node_4826[label="Out"]
   node_4828->node_4827[label="Filter"]
   node_4826->node_4827[label="Input"]
   node_4827->node_4829[label="Output"]
   node_4829->node_4830[label="X"]
   node_4830->node_4831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4865[label="feed"]
   node_4864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4871[label="fetch"]
   node_4870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4868[label="filter_data"]
   node_4866[label="input_data"]
   node_4869[label="output_data"]
   node_4865->node_4864[label="X"]
   node_4864->node_4866[label="Out"]
   node_4868->node_4867[label="Filter"]
   node_4866->node_4867[label="Input"]
   node_4867->node_4869[label="Output"]
   node_4869->node_4870[label="X"]
   node_4870->node_4871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187af1f7b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.802793 0.000000 0.000000 0.000000 0.000000 93929937740796740205017038848.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x3x3         1x1x2x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af1f830>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af18470>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4905[label="feed"]
   node_4904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4911[label="fetch"]
   node_4910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4908[label="filter_data"]
   node_4906[label="input_data"]
   node_4909[label="output_data"]
   node_4905->node_4904[label="X"]
   node_4904->node_4906[label="Out"]
   node_4908->node_4907[label="Filter"]
   node_4906->node_4907[label="Input"]
   node_4907->node_4909[label="Output"]
   node_4909->node_4910[label="X"]
   node_4910->node_4911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4945[label="feed"]
   node_4944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4951[label="fetch"]
   node_4950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4948[label="filter_data"]
   node_4946[label="input_data"]
   node_4949[label="output_data"]
   node_4945->node_4944[label="X"]
   node_4944->node_4946[label="Out"]
   node_4948->node_4947[label="Filter"]
   node_4946->node_4947[label="Input"]
   node_4947->node_4949[label="Output"]
   node_4949->node_4950[label="X"]
   node_4950->node_4951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae98d30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 0.521848 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.802793 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x3x3         1x1x2x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.113041877746582, index : 5.
base=[[[[0.80279344 0.8967006  0.9064706 ]
   [0.74308836 0.8655837  1.1130419 ]]]], 
arr=[[[[8.027934e-01 0.000000e+00 0.000000e+00]
   [0.000000e+00 0.000000e+00 1.761945e-36]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae53070>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae533f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_4987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4985[label="feed"]
   node_4984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4991[label="fetch"]
   node_4990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_4988[label="filter_data"]
   node_4986[label="input_data"]
   node_4989[label="output_data"]
   node_4985->node_4984[label="X"]
   node_4984->node_4986[label="Out"]
   node_4988->node_4987[label="Filter"]
   node_4986->node_4987[label="Input"]
   node_4987->node_4989[label="Output"]
   node_4989->node_4990[label="X"]
   node_4990->node_4991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5025[label="feed"]
   node_5024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5031[label="fetch"]
   node_5030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5028[label="filter_data"]
   node_5026[label="input_data"]
   node_5029[label="output_data"]
   node_5025->node_5024[label="X"]
   node_5024->node_5026[label="Out"]
   node_5028->node_5027[label="Filter"]
   node_5026->node_5027[label="Input"]
   node_5027->node_5029[label="Output"]
   node_5029->node_5030[label="X"]
   node_5030->node_5031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae4fdf0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 3
the chin is 1
the num is 1
the wout is 2
the hout is 3
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x2         1x1x1x1         1x1x3x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae4f270>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae9d470>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5067[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5065[label="feed"]
   node_5064[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5071[label="fetch"]
   node_5070[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5068[label="filter_data"]
   node_5066[label="input_data"]
   node_5069[label="output_data"]
   node_5065->node_5064[label="X"]
   node_5064->node_5066[label="Out"]
   node_5068->node_5067[label="Filter"]
   node_5066->node_5067[label="Input"]
   node_5067->node_5069[label="Output"]
   node_5069->node_5070[label="X"]
   node_5070->node_5071[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5105[label="feed"]
   node_5104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5111[label="fetch"]
   node_5110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5108[label="filter_data"]
   node_5106[label="input_data"]
   node_5109[label="output_data"]
   node_5105->node_5104[label="X"]
   node_5104->node_5106[label="Out"]
   node_5108->node_5107[label="Filter"]
   node_5106->node_5107[label="Input"]
   node_5107->node_5109[label="Output"]
   node_5109->node_5110[label="X"]
   node_5110->node_5111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae53070>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 3
the chin is 1
the num is 1
the wout is 2
the hout is 3
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x2         1x1x1x1         1x1x3x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae575b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae57fb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5145[label="feed"]
   node_5144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5151[label="fetch"]
   node_5150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5148[label="filter_data"]
   node_5146[label="input_data"]
   node_5149[label="output_data"]
   node_5145->node_5144[label="X"]
   node_5144->node_5146[label="Out"]
   node_5148->node_5147[label="Filter"]
   node_5146->node_5147[label="Input"]
   node_5147->node_5149[label="Output"]
   node_5149->node_5150[label="X"]
   node_5150->node_5151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5185[label="feed"]
   node_5184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5191[label="fetch"]
   node_5190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5188[label="filter_data"]
   node_5186[label="input_data"]
   node_5189[label="output_data"]
   node_5185->node_5184[label="X"]
   node_5184->node_5186[label="Out"]
   node_5188->node_5187[label="Filter"]
   node_5186->node_5187[label="Input"]
   node_5187->node_5189[label="Output"]
   node_5189->node_5190[label="X"]
   node_5190->node_5191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae5eeb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 2
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
0.869666 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x2         1x1x2x1         1x1x1x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae5e130>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af1f730>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5225[label="feed"]
   node_5224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5231[label="fetch"]
   node_5230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5228[label="filter_data"]
   node_5226[label="input_data"]
   node_5229[label="output_data"]
   node_5225->node_5224[label="X"]
   node_5224->node_5226[label="Out"]
   node_5228->node_5227[label="Filter"]
   node_5226->node_5227[label="Input"]
   node_5227->node_5229[label="Output"]
   node_5229->node_5230[label="X"]
   node_5230->node_5231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5265[label="feed"]
   node_5264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5271[label="fetch"]
   node_5270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5268[label="filter_data"]
   node_5266[label="input_data"]
   node_5269[label="output_data"]
   node_5265->node_5264[label="X"]
   node_5264->node_5266[label="Out"]
   node_5268->node_5267[label="Filter"]
   node_5266->node_5267[label="Input"]
   node_5267->node_5269[label="Output"]
   node_5269->node_5270[label="X"]
   node_5270->node_5271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae61630>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 2
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 
convolution bias_data == NULL
the outptr distance of outptr and odata is 2 
the output data is 
0.869666 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x2         1x1x2x1         1x1x1x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ade4f30>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ade4ef0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5305[label="feed"]
   node_5304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5311[label="fetch"]
   node_5310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5308[label="filter_data"]
   node_5306[label="input_data"]
   node_5309[label="output_data"]
   node_5305->node_5304[label="X"]
   node_5304->node_5306[label="Out"]
   node_5308->node_5307[label="Filter"]
   node_5306->node_5307[label="Input"]
   node_5307->node_5309[label="Output"]
   node_5309->node_5310[label="X"]
   node_5310->node_5311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5345[label="feed"]
   node_5344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5351[label="fetch"]
   node_5350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5348[label="filter_data"]
   node_5346[label="input_data"]
   node_5349[label="output_data"]
   node_5345->node_5344[label="X"]
   node_5344->node_5346[label="Out"]
   node_5348->node_5347[label="Filter"]
   node_5346->node_5347[label="Input"]
   node_5347->node_5349[label="Output"]
   node_5349->node_5350[label="X"]
   node_5350->node_5351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae5c130>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         1x1x1x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ade84f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ade8cf0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5385[label="feed"]
   node_5384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5391[label="fetch"]
   node_5390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5388[label="filter_data"]
   node_5386[label="input_data"]
   node_5389[label="output_data"]
   node_5385->node_5384[label="X"]
   node_5384->node_5386[label="Out"]
   node_5388->node_5387[label="Filter"]
   node_5386->node_5387[label="Input"]
   node_5387->node_5389[label="Output"]
   node_5389->node_5390[label="X"]
   node_5390->node_5391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5425[label="feed"]
   node_5424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5431[label="fetch"]
   node_5430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5428[label="filter_data"]
   node_5426[label="input_data"]
   node_5429[label="output_data"]
   node_5425->node_5424[label="X"]
   node_5424->node_5426[label="Out"]
   node_5428->node_5427[label="Filter"]
   node_5426->node_5427[label="Input"]
   node_5427->node_5429[label="Output"]
   node_5429->node_5430[label="X"]
   node_5430->node_5431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ade88b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         1x1x1x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001120849609375, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ade48b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ade4130>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5465[label="feed"]
   node_5464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5471[label="fetch"]
   node_5470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5468[label="filter_data"]
   node_5466[label="input_data"]
   node_5469[label="output_data"]
   node_5465->node_5464[label="X"]
   node_5464->node_5466[label="Out"]
   node_5468->node_5467[label="Filter"]
   node_5466->node_5467[label="Input"]
   node_5467->node_5469[label="Output"]
   node_5469->node_5470[label="X"]
   node_5470->node_5471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5505[label="feed"]
   node_5504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5511[label="fetch"]
   node_5510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5508[label="filter_data"]
   node_5506[label="input_data"]
   node_5509[label="output_data"]
   node_5505->node_5504[label="X"]
   node_5504->node_5506[label="Out"]
   node_5508->node_5507[label="Filter"]
   node_5506->node_5507[label="Input"]
   node_5507->node_5509[label="Output"]
   node_5509->node_5510[label="X"]
   node_5510->node_5511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae61d30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 
convolution bias_data == NULL
the outptr distance of outptr and odata is 9 
the output data is 
0.518447 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x3         1x1x1x1         1x1x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001120849609375, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae61070>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ade48b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5545[label="feed"]
   node_5544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5551[label="fetch"]
   node_5550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5548[label="filter_data"]
   node_5546[label="input_data"]
   node_5549[label="output_data"]
   node_5545->node_5544[label="X"]
   node_5544->node_5546[label="Out"]
   node_5548->node_5547[label="Filter"]
   node_5546->node_5547[label="Input"]
   node_5547->node_5549[label="Output"]
   node_5549->node_5550[label="X"]
   node_5550->node_5551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5585[label="feed"]
   node_5584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5591[label="fetch"]
   node_5590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5588[label="filter_data"]
   node_5586[label="input_data"]
   node_5589[label="output_data"]
   node_5585->node_5584[label="X"]
   node_5584->node_5586[label="Out"]
   node_5588->node_5587[label="Filter"]
   node_5586->node_5587[label="Input"]
   node_5587->node_5589[label="Output"]
   node_5589->node_5590[label="X"]
   node_5590->node_5591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ade48b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 
convolution bias_data == NULL
the outptr distance of outptr and odata is 9 
the output data is 
0.518447 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x3         1x1x1x1         1x1x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.03193750000000001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af18470>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af18970>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5625[label="feed"]
   node_5624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5631[label="fetch"]
   node_5630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5628[label="filter_data"]
   node_5626[label="input_data"]
   node_5629[label="output_data"]
   node_5625->node_5624[label="X"]
   node_5624->node_5626[label="Out"]
   node_5628->node_5627[label="Filter"]
   node_5626->node_5627[label="Input"]
   node_5627->node_5629[label="Output"]
   node_5629->node_5630[label="X"]
   node_5630->node_5631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5665[label="feed"]
   node_5664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5671[label="fetch"]
   node_5670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5668[label="filter_data"]
   node_5666[label="input_data"]
   node_5669[label="output_data"]
   node_5665->node_5664[label="X"]
   node_5664->node_5666[label="Out"]
   node_5668->node_5667[label="Filter"]
   node_5666->node_5667[label="Input"]
   node_5667->node_5669[label="Output"]
   node_5669->node_5670[label="X"]
   node_5670->node_5671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187aea76f0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 
convolution bias_data == NULL
the outptr distance of outptr and odata is 9 
the output data is 
0.518447 52324506816490882231271243225497600.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x3         1x1x1x1         1x1x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.03193750000000001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187aea70f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187af18470>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5705[label="feed"]
   node_5704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5711[label="fetch"]
   node_5710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5708[label="filter_data"]
   node_5706[label="input_data"]
   node_5709[label="output_data"]
   node_5705->node_5704[label="X"]
   node_5704->node_5706[label="Out"]
   node_5708->node_5707[label="Filter"]
   node_5706->node_5707[label="Input"]
   node_5707->node_5709[label="Output"]
   node_5709->node_5710[label="X"]
   node_5710->node_5711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5745[label="feed"]
   node_5744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5751[label="fetch"]
   node_5750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5748[label="filter_data"]
   node_5746[label="input_data"]
   node_5749[label="output_data"]
   node_5745->node_5744[label="X"]
   node_5744->node_5746[label="Out"]
   node_5748->node_5747[label="Filter"]
   node_5746->node_5747[label="Input"]
   node_5747->node_5749[label="Output"]
   node_5749->node_5750[label="X"]
   node_5750->node_5751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae4fe30>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ],
         [0.4375872 , 0.891773  , 0.96366274]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 3
the chin is 1
the num is 1
the wout is 3
the hout is 3
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 0.437587 0.891773 0.963663 
the weight data is 

0.944669 
convolution bias_data == NULL
the outptr distance of outptr and odata is 9 
the output data is 
0.518447 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x3x3         1x1x1x1         1x1x3x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 1]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [1, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adf8f30>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187adf8cf0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5785[label="feed"]
   node_5784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5791[label="fetch"]
   node_5790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5788[label="filter_data"]
   node_5786[label="input_data"]
   node_5789[label="output_data"]
   node_5785->node_5784[label="X"]
   node_5784->node_5786[label="Out"]
   node_5788->node_5787[label="Filter"]
   node_5786->node_5787[label="Input"]
   node_5787->node_5789[label="Output"]
   node_5789->node_5790[label="X"]
   node_5790->node_5791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,3,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5825[label="feed"]
   node_5824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5831[label="fetch"]
   node_5830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5828[label="filter_data"]
   node_5826[label="input_data"]
   node_5829[label="output_data"]
   node_5825->node_5824[label="X"]
   node_5824->node_5826[label="Out"]
   node_5828->node_5827[label="Filter"]
   node_5826->node_5827[label="Input"]
   node_5827->node_5829[label="Output"]
   node_5829->node_5830[label="X"]
   node_5830->node_5831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adfc070>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ],
         [0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 3
the chin is 1
the num is 1
the wout is 2
the hout is 2
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 4 
the output data is 
0.951474 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x3x2         1x1x2x1         1x1x2x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.030584454536438, index : 1.
base=[[[[0.9514744 1.0305845]
   [0.8376292 0.9859403]]]], 
arr=[[[[9.5147443e-01 2.1802235e-36]
   [1.4012985e-45 1.9642764e-36]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 1]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [1, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187af1fe70>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187aed34b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5865[label="feed"]
   node_5864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5871[label="fetch"]
   node_5870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5868[label="filter_data"]
   node_5866[label="input_data"]
   node_5869[label="output_data"]
   node_5865->node_5864[label="X"]
   node_5864->node_5866[label="Out"]
   node_5868->node_5867[label="Filter"]
   node_5866->node_5867[label="Input"]
   node_5867->node_5869[label="Output"]
   node_5869->node_5870[label="X"]
   node_5870->node_5871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5905[label="feed"]
   node_5904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5911[label="fetch"]
   node_5910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5908[label="filter_data"]
   node_5906[label="input_data"]
   node_5909[label="output_data"]
   node_5905->node_5904[label="X"]
   node_5904->node_5906[label="Out"]
   node_5908->node_5907[label="Filter"]
   node_5906->node_5907[label="Input"]
   node_5907->node_5909[label="Output"]
   node_5909->node_5910[label="X"]
   node_5910->node_5911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae611b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.901118 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x3         1x1x2x1         1x1x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.030980110168457, index : 2.
base=[[[[0.901118  0.9251142 1.0309801]]]], 
arr=[[[[0.9011179 0.        0.       ]]]][0m
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adeccb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae4fa30>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5945[label="feed"]
   node_5944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5951[label="fetch"]
   node_5950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5948[label="filter_data"]
   node_5946[label="input_data"]
   node_5949[label="output_data"]
   node_5945->node_5944[label="X"]
   node_5944->node_5946[label="Out"]
   node_5948->node_5947[label="Filter"]
   node_5946->node_5947[label="Input"]
   node_5947->node_5949[label="Output"]
   node_5949->node_5950[label="X"]
   node_5950->node_5951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_5987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5985[label="feed"]
   node_5984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5991[label="fetch"]
   node_5990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_5988[label="filter_data"]
   node_5986[label="input_data"]
   node_5989[label="output_data"]
   node_5985->node_5984[label="X"]
   node_5984->node_5986[label="Out"]
   node_5988->node_5987[label="Filter"]
   node_5986->node_5987[label="Input"]
   node_5987->node_5989[label="Output"]
   node_5989->node_5990[label="X"]
   node_5990->node_5991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adf8cb0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 2
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.877940 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x1         1x1x2x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae19530>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187adeccb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6025[label="feed"]
   node_6024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6031[label="fetch"]
   node_6030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6028[label="filter_data"]
   node_6026[label="input_data"]
   node_6029[label="output_data"]
   node_6025->node_6024[label="X"]
   node_6024->node_6026[label="Out"]
   node_6028->node_6027[label="Filter"]
   node_6026->node_6027[label="Input"]
   node_6027->node_6029[label="Output"]
   node_6029->node_6030[label="X"]
   node_6030->node_6031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,1}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6067[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6065[label="feed"]
   node_6064[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6071[label="fetch"]
   node_6070[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6068[label="filter_data"]
   node_6066[label="input_data"]
   node_6069[label="output_data"]
   node_6065->node_6064[label="X"]
   node_6064->node_6066[label="Out"]
   node_6068->node_6067[label="Filter"]
   node_6066->node_6067[label="Input"]
   node_6067->node_6069[label="Output"]
   node_6069->node_6070[label="X"]
   node_6070->node_6071[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae61870>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 ],
         [0.71518934]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 1
the hin is 2
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 
the weight data is 

0.437587 0.891773 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
0.877940 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x1         1x1x2x1         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae224b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae22bf0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6107[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6105[label="feed"]
   node_6104[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6111[label="fetch"]
   node_6110[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6108[label="filter_data"]
   node_6106[label="input_data"]
   node_6109[label="output_data"]
   node_6105->node_6104[label="X"]
   node_6104->node_6106[label="Out"]
   node_6108->node_6107[label="Filter"]
   node_6106->node_6107[label="Input"]
   node_6107->node_6109[label="Output"]
   node_6109->node_6110[label="X"]
   node_6110->node_6111[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6147[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6145[label="feed"]
   node_6144[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6151[label="fetch"]
   node_6150[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6148[label="filter_data"]
   node_6146[label="input_data"]
   node_6149[label="output_data"]
   node_6145->node_6144[label="X"]
   node_6144->node_6146[label="Out"]
   node_6148->node_6147[label="Filter"]
   node_6146->node_6147[label="Input"]
   node_6147->node_6149[label="Output"]
   node_6149->node_6150[label="X"]
   node_6150->node_6151[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae22d70>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 2
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
1.064021 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x2p0s1g1d1unk             1x1x2x2         1x1x2x2         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae228b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae224b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6187[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6185[label="feed"]
   node_6184[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6191[label="fetch"]
   node_6190[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6188[label="filter_data"]
   node_6186[label="input_data"]
   node_6189[label="output_data"]
   node_6185->node_6184[label="X"]
   node_6184->node_6186[label="Out"]
   node_6188->node_6187[label="Filter"]
   node_6186->node_6187[label="Input"]
   node_6187->node_6189[label="Output"]
   node_6189->node_6190[label="X"]
   node_6190->node_6191[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,1}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6227[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6225[label="feed"]
   node_6224[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6231[label="fetch"]
   node_6230[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6228[label="filter_data"]
   node_6226[label="input_data"]
   node_6229[label="output_data"]
   node_6225->node_6224[label="X"]
   node_6224->node_6226[label="Out"]
   node_6228->node_6227[label="Filter"]
   node_6226->node_6227[label="Input"]
   node_6227->node_6229[label="Output"]
   node_6229->node_6230[label="X"]
   node_6230->node_6231[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ada5df0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 1
the hout is 1
the chout is 1
the kw is 2
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 0.925597 0.071036 0.087129 
convolution bias_data == NULL
the outptr distance of outptr and odata is 1 
the output data is 
1.064021 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x2p0s1g1d1unk             1x1x2x2         1x1x2x2         1x1x1x1         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae98fb0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae983b0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6267[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6265[label="feed"]
   node_6264[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6271[label="fetch"]
   node_6270[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6268[label="filter_data"]
   node_6266[label="input_data"]
   node_6269[label="output_data"]
   node_6265->node_6264[label="X"]
   node_6264->node_6266[label="Out"]
   node_6268->node_6267[label="Filter"]
   node_6266->node_6267[label="Input"]
   node_6267->node_6269[label="Output"]
   node_6269->node_6270[label="X"]
   node_6270->node_6271[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6307[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6305[label="feed"]
   node_6304[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6311[label="fetch"]
   node_6310[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6308[label="filter_data"]
   node_6306[label="input_data"]
   node_6309[label="output_data"]
   node_6305->node_6304[label="X"]
   node_6304->node_6306[label="Out"]
   node_6308->node_6307[label="Filter"]
   node_6306->node_6307[label="Input"]
   node_6307->node_6309[label="Output"]
   node_6309->node_6310[label="X"]
   node_6310->node_6311[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae005b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 2
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 
convolution bias_data == NULL
the outptr distance of outptr and odata is 4 
the output data is 
0.311751 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x2         1x1x1x1         1x1x2x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae00f30>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae98fb0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6347[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6345[label="feed"]
   node_6344[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6351[label="fetch"]
   node_6350[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6348[label="filter_data"]
   node_6346[label="input_data"]
   node_6349[label="output_data"]
   node_6345->node_6344[label="X"]
   node_6344->node_6346[label="Out"]
   node_6348->node_6347[label="Filter"]
   node_6346->node_6347[label="Input"]
   node_6347->node_6349[label="Output"]
   node_6349->node_6350[label="X"]
   node_6350->node_6351[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,2}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6387[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6385[label="feed"]
   node_6384[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6391[label="fetch"]
   node_6390[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6388[label="filter_data"]
   node_6386[label="input_data"]
   node_6389[label="output_data"]
   node_6385->node_6384[label="X"]
   node_6384->node_6386[label="Out"]
   node_6388->node_6387[label="Filter"]
   node_6386->node_6387[label="Input"]
   node_6387->node_6389[label="Output"]
   node_6389->node_6390[label="X"]
   node_6390->node_6391[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae5c1b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934],
         [0.60276335, 0.5448832 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 2
the hin is 2
the chin is 1
the num is 1
the wout is 2
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 
the weight data is 

0.568045 
convolution bias_data == NULL
the outptr distance of outptr and odata is 4 
the output data is 
0.311751 9938503000725230723621466406912.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x2         1x1x1x1         1x1x2x2         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ade8ab0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ade8a70>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6427[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6425[label="feed"]
   node_6424[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6431[label="fetch"]
   node_6430[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6428[label="filter_data"]
   node_6426[label="input_data"]
   node_6429[label="output_data"]
   node_6425->node_6424[label="X"]
   node_6424->node_6426[label="Out"]
   node_6428->node_6427[label="Filter"]
   node_6426->node_6427[label="Input"]
   node_6427->node_6429[label="Output"]
   node_6429->node_6430[label="X"]
   node_6430->node_6431[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6467[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6465[label="feed"]
   node_6464[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6471[label="fetch"]
   node_6470[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6468[label="filter_data"]
   node_6466[label="input_data"]
   node_6469[label="output_data"]
   node_6465->node_6464[label="X"]
   node_6464->node_6466[label="Out"]
   node_6468->node_6467[label="Filter"]
   node_6466->node_6467[label="Input"]
   node_6467->node_6469[label="Output"]
   node_6469->node_6470[label="X"]
   node_6470->node_6471[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ae611b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 1
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 
the weight data is 

0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.210438 3392009807072369707879168475136.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x3         1x1x1x1         1x1x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae617b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ade8ab0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6507[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6505[label="feed"]
   node_6504[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6511[label="fetch"]
   node_6510[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6508[label="filter_data"]
   node_6506[label="input_data"]
   node_6509[label="output_data"]
   node_6505->node_6504[label="X"]
   node_6504->node_6506[label="Out"]
   node_6508->node_6507[label="Filter"]
   node_6506->node_6507[label="Input"]
   node_6507->node_6509[label="Output"]
   node_6509->node_6510[label="X"]
   node_6510->node_6511[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6547[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6545[label="feed"]
   node_6544[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6551[label="fetch"]
   node_6550[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6548[label="filter_data"]
   node_6546[label="input_data"]
   node_6549[label="output_data"]
   node_6545->node_6544[label="X"]
   node_6544->node_6546[label="Out"]
   node_6548->node_6547[label="Filter"]
   node_6546->node_6547[label="Input"]
   node_6547->node_6549[label="Output"]
   node_6549->node_6550[label="X"]
   node_6550->node_6551[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187ade8ab0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 1
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 
the weight data is 

0.383442 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.210438 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x1x3         1x1x1x1         1x1x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 1]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [1, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001120849609375, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187ae224f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae228f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6587[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6585[label="feed"]
   node_6584[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6591[label="fetch"]
   node_6590[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6588[label="filter_data"]
   node_6586[label="input_data"]
   node_6589[label="output_data"]
   node_6585->node_6584[label="X"]
   node_6584->node_6586[label="Out"]
   node_6588->node_6587[label="Filter"]
   node_6586->node_6587[label="Input"]
   node_6587->node_6589[label="Output"]
   node_6589->node_6590[label="X"]
   node_6590->node_6591[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6627[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6625[label="feed"]
   node_6624[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6631[label="fetch"]
   node_6630[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6628[label="filter_data"]
   node_6626[label="input_data"]
   node_6629[label="output_data"]
   node_6625->node_6624[label="X"]
   node_6624->node_6626[label="Out"]
   node_6628->node_6627[label="Filter"]
   node_6626->node_6627[label="Input"]
   node_6627->node_6629[label="Output"]
   node_6629->node_6630[label="X"]
   node_6630->node_6631[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adb60b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         1x1x1x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001120849609375, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adb62b0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187adf8bf0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6667[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6665[label="feed"]
   node_6664[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6671[label="fetch"]
   node_6670[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6668[label="filter_data"]
   node_6666[label="input_data"]
   node_6669[label="output_data"]
   node_6665->node_6664[label="X"]
   node_6664->node_6666[label="Out"]
   node_6668->node_6667[label="Filter"]
   node_6666->node_6667[label="Input"]
   node_6667->node_6669[label="Output"]
   node_6669->node_6670[label="X"]
   node_6670->node_6671[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.00112085
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6707[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6705[label="feed"]
   node_6704[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6711[label="fetch"]
   node_6710[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6708[label="filter_data"]
   node_6706[label="input_data"]
   node_6709[label="output_data"]
   node_6705->node_6704[label="X"]
   node_6704->node_6706[label="Out"]
   node_6708->node_6707[label="Filter"]
   node_6706->node_6707[label="Input"]
   node_6707->node_6709[label="Output"]
   node_6709->node_6710[label="X"]
   node_6710->node_6711[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adf8bf0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         1x1x1x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.03193750000000001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adb79f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187adb7130>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6747[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6745[label="feed"]
   node_6744[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6751[label="fetch"]
   node_6750[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6748[label="filter_data"]
   node_6746[label="input_data"]
   node_6749[label="output_data"]
   node_6745->node_6744[label="X"]
   node_6744->node_6746[label="Out"]
   node_6748->node_6747[label="Filter"]
   node_6746->node_6747[label="Input"]
   node_6747->node_6749[label="Output"]
   node_6749->node_6750[label="X"]
   node_6750->node_6751[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6787[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6785[label="feed"]
   node_6784[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6791[label="fetch"]
   node_6790[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6788[label="filter_data"]
   node_6786[label="input_data"]
   node_6789[label="output_data"]
   node_6785->node_6784[label="X"]
   node_6784->node_6786[label="Out"]
   node_6788->node_6787[label="Filter"]
   node_6786->node_6787[label="Input"]
   node_6787->node_6789[label="Output"]
   node_6789->node_6790[label="X"]
   node_6790->node_6791[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adbe4b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         1x1x1x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.03193750000000001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adbebf0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187adb79f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6827[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6825[label="feed"]
   node_6824[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6831[label="fetch"]
   node_6830[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6828[label="filter_data"]
   node_6826[label="input_data"]
   node_6829[label="output_data"]
   node_6825->node_6824[label="X"]
   node_6824->node_6826[label="Out"]
   node_6828->node_6827[label="Filter"]
   node_6826->node_6827[label="Input"]
   node_6827->node_6829[label="Output"]
   node_6829->node_6830[label="X"]
   node_6830->node_6831[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.0319375
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6867[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6865[label="feed"]
   node_6864[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6871[label="fetch"]
   node_6870[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6868[label="filter_data"]
   node_6866[label="input_data"]
   node_6869[label="output_data"]
   node_6865->node_6864[label="X"]
   node_6864->node_6866[label="Out"]
   node_6868->node_6867[label="Filter"]
   node_6866->node_6867[label="Input"]
   node_6867->node_6869[label="Output"]
   node_6869->node_6870[label="X"]
   node_6870->node_6871[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adbd970>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 2
the chout is 1
the kw is 1
the kh is 1
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 
convolution bias_data == NULL
the outptr distance of outptr and odata is 6 
the output data is 
0.427063 0.000000 0.000000 0.000000 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      1x1p0s1g1d1unk             1x1x2x3         1x1x1x1         1x1x2x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]
In TestConv2dOp sample_program_configs printf paddings is [0, 0]
In TestConv2dOp sample_program_configs printf paddings is [1, 1]

Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001120849609375, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.03193750000000001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 3, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Traceback (most recent call last):
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
  File "/usr/lib/python3.7/unittest/case.py", line 705, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true

Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 2), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 1, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001120849609375, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Trying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.03193750000000001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 1, 1), 'lod': None, 'dtype': dtype('float32')}],
)
Falsifying example: run_test(
    prog_config=conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}],
)py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
py CxxConfig class 's set_valid_places will be running
program_config.py CxxConfig set_valid_places is running
py CxxConfig class 's set_threads will be running
program_config.py CxxConfig set_threads is running
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adbeab0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187adbe870>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 1 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6907[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6905[label="feed"]
   node_6904[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6911[label="fetch"]
   node_6910[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6908[label="filter_data"]
   node_6906[label="input_data"]
   node_6909[label="output_data"]
   node_6905->node_6904[label="X"]
   node_6904->node_6906[label="Out"]
   node_6908->node_6907[label="Filter"]
   node_6906->node_6907[label="Input"]
   node_6907->node_6909[label="Output"]
   node_6909->node_6910[label="X"]
   node_6910->node_6911[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6947[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6945[label="feed"]
   node_6944[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6951[label="fetch"]
   node_6950[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6948[label="filter_data"]
   node_6946[label="input_data"]
   node_6949[label="output_data"]
   node_6945->node_6944[label="X"]
   node_6944->node_6946[label="Out"]
   node_6948->node_6947[label="Filter"]
   node_6946->node_6947[label="Input"]
   node_6947->node_6949[label="Output"]
   node_6949->node_6950[label="X"]
   node_6950->node_6951[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adb71b0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.901118 0.000000 2935472873941830568286890229760.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x3         1x1x2x1         1x1x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

SUCCESS: PredictorConfig: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 1}
the atol_ is
the rtol_ is
[ProgramConfig]: conv2d{'strides': [1, 1], 'paddings': [0, 0], 'use_mkldnn': False, 'padding_algorithm': 'VALID', 'groups': 1, 'dilations': [1, 1], 'Scale_in': 0.001, 'Scale_out': 0.001, 'data_format': 'NCHW'} -- [input_data: {'shape': (1, 1, 2, 3), 'lod': None, 'dtype': dtype('float32')}][filter_data: {'shape': (1, 1, 2, 1), 'lod': None, 'dtype': dtype('float32')}]
In AutoScanBaseTest class 's run_test_config function pred_config is <paddle.fluid.core_avx.AnalysisConfig object at 0x7f187adb71f0>
In AutoScanBaseTest class 's run_test_config function will be invoking paddle_infer.create_predictor <built-in method create_predictor of PyCapsule object at 0x7f189491ef60>
In AutoScanBaseTest class 's run_test_config function predictor is <paddle.fluid.core_avx.PaddleInferPredictor object at 0x7f187ae4f8f0>
In AutoScanBaseTest class 's run_test_config function self.available_passes_in_framework is {'is_test_pass', 'conv_eltwiseadd_bn_fuse_pass', 'seqconv_eltadd_relu_fuse_pass', 'matmul_v2_scale_fuse_pass', 'gpu_cpu_map_matmul_to_mul_pass', 'matmul_scale_fuse_pass', 'attention_lstm_fuse_pass', 'squared_mat_sub_fuse_pass', 'seqpool_cvm_concat_fuse_pass', 'gpu_cpu_reshape2_matmul_fuse_pass', 'gpu_cpu_flatten2_matmul_fuse_pass', 'gpu_cpu_map_matmul_v2_to_matmul_pass', 'graph_viz_pass', 'simplify_with_basic_ops_pass', 'seq_concat_fc_fuse_pass', 'conv_transpose_eltwiseadd_bn_fuse_pass', 'gpu_cpu_map_matmul_v2_to_mul_pass', 'conv_transpose_bn_fuse_pass', 'mul_gru_fuse_pass', 'layer_norm_fuse_pass', 'gpu_cpu_squeeze2_matmul_fuse_pass', 'runtime_context_cache_pass', 'mul_lstm_fuse_pass', 'repeated_fc_relu_fuse_pass', 'fc_gru_fuse_pass', 'fc_fuse_pass', 'conv_bn_fuse_pass'}
py The AutoScanTest ParsePaddleLiteConfig function new CxxConfig the default constructor is running
ParsePaddleLiteConfig function place_str is X86,FP32,NCHW
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxConfig class base class 's set_valid_places is running and then print valid_places_
ConfigBase class base class 's set_threads is running and threads is 4 
CxxModelBuffer::CxxModelBuffer function program_buffer is : 
“ 
CxxModelBuffer::CxxModelBuffer function params_ is :  
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_6987[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6985[label="feed"]
   node_6984[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6991[label="fetch"]
   node_6990[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_6988[label="filter_data"]
   node_6986[label="input_data"]
   node_6989[label="output_data"]
   node_6985->node_6984[label="X"]
   node_6984->node_6986[label="Out"]
   node_6988->node_6987[label="Filter"]
   node_6986->node_6987[label="Input"]
   node_6987->node_6989[label="Output"]
   node_6989->node_6990[label="X"]
   node_6990->node_6991[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
In pybind.cc create_paddle_predictor of CxxConfig 
In ConfigBase constructor which have 2 params on the LITE_WITH_ARM 
CxxPaddleApiImpl class Init function is running 
CXXPaddleApiImpl class Init function status_is_cloned_ is false 
attention LOG command
use_layout_preprocess_pass:18446744073709551615
Pass in a value greater than 1.0 to turn off the sparse pass internally
CXXPaddleApiImpl class Init function raw_predictor_->Build will be running 
Predictor::Build which have 3 params is running the first param is CxxConfig
Load model from memory.
Predictor::Build which have 7 params is running 
model_type is lite_api::LiteModelType::kProtobuf 
In LoadModelPb function TransformProgramDescAnyToCpp(pb_prog, cpp_prog) will be running 
IN LoadModelPb function macro WITH_CONVERT_TO_SSA is ON 
general::ssa::ConvertToSSA(cpp_prog) will be running 
ConvertToSSA(general::ProgramDesc* prog) which will be return general::ProgramDesc* type is running 
The model_parser.cc file LoadCombinedParamsPb function input argument scope is nullptr. 
Predictor::Build which have 3 params and the first param is ProgramDesc is running 
Predictor::Build `inner_places` is used to optimize passes 
prepare work
Program::PrepareWorkspace(program_desc, vars_to_clone) is running 
Var feed in block 0
 - type 12
Var fetch in block 0
 - type 13
Var filter_data in block 0
 - type 10
 - data type 1
Var input_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,2,3}
Var output_data in block 0
 - type 10
 - data type 1
data type 1
 - dims {1,1,1,3}
build desc
Program::Build(program_desc) is running 
create Op [feed]
create Op [conv2d]
create Op [fetch]
build desc finished
RunDefaultOptimizer() will be running 
RunDefaultOptimizer is running 
class Optimizer construction is running 
Optimizer::Run() is running 
pick kernel for feed host/float/NCHW get 0 kernels
pick kernel for feed host/float/any get 0 kernels
pick kernel for feed host/any/NCHW get 0 kernels
pick kernel for feed host/any/any get 1 kernels
pick kernel for feed x86/float/NCHW get 0 kernels
pick kernel for feed x86/float/any get 0 kernels
pick kernel for feed x86/any/NCHW get 0 kernels
pick kernel for feed x86/any/any get 0 kernels
pick kernel for conv2d host/float/NCHW get 0 kernels
pick kernel for conv2d host/float/any get 0 kernels
pick kernel for conv2d host/any/NCHW get 0 kernels
pick kernel for conv2d host/any/any get 0 kernels
pick kernel for conv2d x86/float/NCHW get 1 kernels
pick kernel for conv2d x86/float/any get 0 kernels
pick kernel for conv2d x86/any/NCHW get 0 kernels
pick kernel for conv2d x86/any/any get 0 kernels
pick kernel for fetch host/float/NCHW get 0 kernels
pick kernel for fetch host/float/any get 0 kernels
pick kernel for fetch host/any/NCHW get 0 kernels
pick kernel for fetch host/any/any get 1 kernels
pick kernel for fetch x86/float/NCHW get 0 kernels
pick kernel for fetch x86/float/any get 0 kernels
pick kernel for fetch x86/any/NCHW get 0 kernels
pick kernel for fetch x86/any/any get 0 kernels
   - Skip op_transformation_pass because the target or kernel does not match.
   - Skip remove_scale1_pass because the target or kernel does not match.
   - Skip assign_value_calc_offline_pass because the target or kernel does not match.
   - Skip p_norm_fill_constant_max_div_fuse_pass because the target or kernel does not match.
   - Skip fill_constant_calc_offline_pass because the target or kernel does not match.
   - Skip range_calc_offline_pass because the target or kernel does not match.
   - Skip scale_calc_offline_pass because the target or kernel does not match.
   - Skip unsqueeze_calc_offline_pass because the target or kernel does not match.
   - Skip ssd_boxes_calc_offline_pass because the target or kernel does not match.
   - Skip adaptive_1x1_pool2d_convert_global_pass because the target or kernel does not match.
   - Skip lite_var_conv_2d_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_match_matrix_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_reverse_embedding_fuse_pass because the target or kernel does not match.
   - Skip lite_sequence_pool_concat_fuse_pass because the target or kernel does not match.
   - Skip lite_scale_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_scaleacts_fuse_pass because the target or kernel does not match.
   - Skip lite_elementwise_scale_fuse_pass because the target or kernel does not match.
   - Skip lite_instance_norm_activation_fuse_pass because the target or kernel does not match.
   - Skip lite_fc_prelu_fuse_pass because the target or kernel does not match.
   - Skip lite_conv_elementwise_tree_fuse_pass because the target or kernel does not match.
   - Skip lite_greater_than_cast_fuse_pass because the target or kernel does not match.
   - Skip fill_range_fuse_pass because the target or kernel does not match.
   - Skip sparse_conv_detect_pass because the target or kernel does not match.
   - Skip keepdims_convert_pass because the target or kernel does not match.
   - Skip __xpu__max_pooling_pad_zero_detect_fuse_pass because the target or kernel does not match.
   - Skip __xpu__graph_dedup_pass because the target or kernel does not match.
   - Skip __xpu__resnet_fuse_pass because the target or kernel does not match.
   - Skip __xpu__resnet_cbam_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_affine_channel_fuse_pass because the target or kernel does not match.
   - Skip __xpu__conv2d_fuse_pass because the target or kernel does not match.
   - Skip __xpu__squeeze_excitation_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_meanstd_fuse_pass because the target or kernel does not match.
   - Skip __xpu__sfa_head_moment_fuse_pass because the target or kernel does not match.
   - Skip __xpu__mmdnn_fuse_pass because the target or kernel does not match.
   - Skip __xpu__bigru_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_fuse_pass because the target or kernel does not match.
   - Skip __xpu__embedding_with_eltwise_add_fuse_pass because the target or kernel does not match.
   - Skip __xpu__fc_fuse_pass because the target or kernel does not match.
   - Skip __xpu__softmax_topk_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_adaptive_seqlen_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_encoder_slice_link_fuse_pass because the target or kernel does not match.
   - Skip __xpu__generate_sequence_fuse_pass because the target or kernel does not match.
   - Skip __xpu__logit_fuse_pass because the target or kernel does not match.
   - Skip __xpu__link_previous_out_max_pass because the target or kernel does not match.
   - Skip fix_mismatched_precision_pass because the target or kernel does not match.
   - Skip __xpu__dynamic_lstm_fuse_pass because the target or kernel does not match.
   - Skip __xpu__multi_softmax_fuse_pass because the target or kernel does not match.
   - Skip quantization_parameters_propagation_pass because the target or kernel does not match.
   - Skip restrict_quantized_op_with_same_input_output_scale_pass because the target or kernel does not match.
   - Skip quantization_parameters_removal_pass because the target or kernel does not match.
   - Skip nnadapter_subgraph_pass because the target or kernel does not match.
   - Skip npu_subgraph_pass because the target or kernel does not match.
   - Skip bm_subgraph_pass because the target or kernel does not match.
   - Skip mlu_subgraph_pass because the target or kernel does not match.
   - Skip fpga_concat_fuse_pass because the target or kernel does not match.
   - Skip control_flow_op_unused_inputs_and_outputs_eliminate_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip __fpga_kernel_place_correct_pass because the target or kernel does not match.
   - Skip mlu_postprocess_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip control_flow_op_shared_inputs_and_outputs_place_sync_pass because the target or kernel does not match.
   - Skip memory_optimize_pass because the target or kernel does not match.
   - Skip xpu_memory_optimize_pass because the target or kernel does not match.
final program 
* feed0
 - col:int:0
* conv2d1
 - Scale_in:float:0.001
 - Scale_in_eltwise:float:1
 - Scale_out:float:0.001
 - Scale_weights:floats: {1}
 - data_format:string: "NCHW"
 - dilations:ints: {1,1}
 - exhaustive_search:int:0
 - force_fp32_output:int:0
 - fuse_activation:string: ""
 - fuse_alpha:float:0
 - fuse_beta:float:0
 - fuse_brelu:int:0
 - fuse_brelu_threshold:float:6
 - fuse_relu:int:0
 - fuse_relu_before_depthwise_conv:int:0
 - fuse_residual_connection:int:0
 - groups:int:1
 - is_test:int:0
 - mkldnn_data_type:string: "float32"
 - op_device:string: ""
 - padding_algorithm:string: "VALID"
 - paddings:ints: {0,0}
 - strides:ints: {1,1}
 - use_addto:int:0
 - use_cudnn:int:0
 - use_mkldnn:int:0
 - use_quantizer:int:0
 - with_quant_attr:int:0
* fetch2
 - col:int:0
 - data_type:int:1
digraph G {
   node_7027[label="conv2d1" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_7025[label="feed"]
   node_7024[label="feed0" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_7031[label="fetch"]
   node_7030[label="fetch2" shape="box" style="filled" color="black" fillcolor="yellow"]
   node_7028[label="filter_data"]
   node_7026[label="input_data"]
   node_7029[label="output_data"]
   node_7025->node_7024[label="X"]
   node_7024->node_7026[label="Out"]
   node_7028->node_7027[label="Filter"]
   node_7026->node_7027[label="Input"]
   node_7027->node_7029[label="Output"]
   node_7029->node_7030[label="X"]
   node_7030->node_7031[label="Out"]
} // end G
In GenerateProgramPass::Apply function the SSAGraph Visualize is finishing 
Statement feed host/any/any
Statement conv2d x86/float/NCHW
Statement fetch host/any/any
The AutoScanTest class run_lite_config function create_paddle_predictor is <paddlelite.lite.CxxPredictor object at 0x7f187adbeab0>
The AutoScanTest class run_lite_config function create_paddle_predictor is <built-in method create_paddle_predictor of PyCapsule object at 0x7f187d6da840>
The AutoScanTest class run_lite_config function predictor.get_input_names() elem is input_data
The AutoScanTest class run_lite_config function predictor.get_output_names() elem is output_data
The AutoScanTest class run_lite_config function inputs is {'input_data': {'data': array([[[[0.5488135 , 0.71518934, 0.60276335],
         [0.5448832 , 0.4236548 , 0.6458941 ]]]], dtype=float32), 'lod': None}}
The AutoScanTest class run_lite_config function inputs elem name is input_data
LITE_WITH_X86 PADDLE_WITH_MKLML not LITE_ON_MODEL_OPTIMIZE_TOOL is openning 
Conv2dCompute<float,float> PrepareForRun is running 
Conv2dCompute<float float> Run is running1
conv_compute.cc init01 
conv_compute.cc init02 
conv_compute.cc init1 
the win is 3
the hin is 2
the chin is 1
the num is 1
the wout is 3
the hout is 1
the chout is 1
the kw is 1
the kh is 2
the group is 1
Conv2dCompute<float float> Run is running2
the paddings is 
0 0 0 0 Conv2dCompute<float float> Run is running3
the dilation_w is 1 
the dilation_h is 1 
Conv2dCompute<float float> Run is running4
the input data is 

0.548814 0.715189 0.602763 0.544883 0.423655 0.645894 
the weight data is 

0.778157 0.870012 
convolution bias_data == NULL
the outptr distance of outptr and odata is 3 
the output data is 
0.901118 0.000000 0.000000 
In RuntimeProgram::Run() is running the LITE_WITH_PROFILE is open 

===== Detailed Dispatch Profiler Summary: N/A, Exclude 1 warm-ups =====
OperatorType         KerneAttr(Place)               KernelFuncName           Remark                     InDim           FilterDim       OutDim          Avg(ms) Min(ms) Max(ms) Last(ms) Avg(%)  GOPs    GOPS   
conv2d               x86/float/NCHW                 N/A                      2x1p0s1g1d1unk             1x1x2x3         1x1x2x1         1x1x1x3         0.000   0.000   0.000   0.000   0.00%    0.000   inf    

FAILE: {'discarded_passes': [], 'valid_targets': ['X86,FP32,NCHW'], 'thread': 4}[1;31m 
ERROR INFO: False is not true : Output has diff, max_diff : 1.030980110168457, index : 2.
base=[[[[0.901118  0.9251142 1.0309801]]]], 
arr=[[[[9.0111792e-01 2.3851074e-36 1.4012985e-45]]]][0m
F
======================================================================
FAIL: test (__main__.TestConv2dOp)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test_conv2d_op.py", line 222, in test
    self.run_and_statis(quant=False, max_examples=25)
  File "../auto_scan_base.py", line 779, in run_and_statis
    loop_func()
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "/home/ubuntu/.local/lib/python3.7/site-packages/hypothesis/core.py", line 1197, in wrapped_test
    raise the_error_hypothesis_found
  File "../auto_scan_base.py", line 743, in run_test
    return self.run_test(quant=quant, prog_configs=[prog_config])
  File "../auto_scan_base.py", line 596, in run_test
    self.assertTrue(status)
AssertionError: False is not true

----------------------------------------------------------------------
Ran 1 test in 7.279s

FAILED (failures=1)


You can reproduce this example by temporarily adding @reproduce_failure('6.27.0', b'AXicY2BgYGRiYGTABAAAcQAF') as a decorator on your test case
